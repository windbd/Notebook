{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "- huggingface下transformers库的分词工具：Tokenizer快速使用\n",
    "- 安装torchtext时需要考虑对应版本的问题：\n",
    "  - pytroch的的版本为1.a.b，则torchtext的版本为0.(a+1).b\n",
    "  - 若pytorch == 1.13.1，则pip install torchtext == 0.14.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18778a12-8887-43bc-a6ae-f1cac4775650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "embed = torch.nn.Embedding(num_embeddings=10, embedding_dim=4)#[10,4]\n",
    "# word2id的作用，1表示EOS，2表示PAD\n",
    "batch = [[3, 6, 5, 6, 7, 1], [6, 4, 7, 9, 5, 1], [4, 5, 8, 7, 1, 2]]#[3,6]\n",
    "batch = torch.LongTensor(batch)\n",
    "batch = batch.reshape(6, 3)  # [seq_len,batch_size]排序>>为了在RNN模型中进行训练\n",
    "batch_embed = embed(batch)  # [6,3,10][10,4]=[6,3,4]，不能超过10类\n",
    "batch_embed.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV\n",
    "- Pytorch Image Models (timm)有常用的视觉模型：Pytorch视觉模型库--timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbf210c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace=True)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 7, 7])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 加载CV领域的模型结构（方法一）\n",
    "\n",
    "# 参考链接：https://blog.csdn.net/me_yundou/article/details/109218273\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# resnet = torchvision.models.resnet50(pretrained=False)#无预训练参数\n",
    "model = torchvision.models.vgg19(pretrained=True)\n",
    "print(model)#打印网络结构\n",
    "\n",
    "# model.classifier.add_module(\"add_linear\",nn.Linear(1000,10)) # 在resnet50的classfier里加一层\n",
    "# model.classifier[6] = nn.Linear(4096,10) # 修改对应层,编号相对应\n",
    "\n",
    "# model=list(model.children())[:-1]#去掉后一层只保留（2048/512，7，7）\n",
    "# model = torch.nn.Sequential(*model)\n",
    "\n",
    "# input = torch.randn(2,3,224,224)\n",
    "# output = model(input)\n",
    "# output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3646f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加载CVCV领域的模型结构（方法二）\n",
    "\n",
    "import timm\n",
    "model = timm.create_model('vgg19',pretrained=True)\n",
    "# model = timm.create_model(\"hf_hub:timm/vgg19.tv_in1k\", pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2805ff7",
   "metadata": {},
   "source": [
    "# pytorch\n",
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3613bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchkeras import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "621d33de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "  (adaptive_pool): AdaptiveMaxPool2d(output_size=(1, 1))\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (linear2): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "--------------------------------------------------------------------------\n",
      "Layer (type)                            Output Shape              Param #\n",
      "==========================================================================\n",
      "Conv2d-1                            [-1, 32, 30, 30]                  896\n",
      "MaxPool2d-2                         [-1, 32, 15, 15]                    0\n",
      "Conv2d-3                            [-1, 64, 11, 11]               51,264\n",
      "MaxPool2d-4                           [-1, 64, 5, 5]                    0\n",
      "Dropout2d-5                           [-1, 64, 5, 5]                    0\n",
      "AdaptiveMaxPool2d-6                   [-1, 64, 1, 1]                    0\n",
      "Flatten-7                                   [-1, 64]                    0\n",
      "Linear-8                                    [-1, 32]                2,080\n",
      "ReLU-9                                      [-1, 32]                    0\n",
      "Linear-10                                    [-1, 1]                   33\n",
      "Sigmoid-11                                   [-1, 1]                    0\n",
      "==========================================================================\n",
      "Total params: 54,273\n",
      "Trainable params: 54,273\n",
      "Non-trainable params: 0\n",
      "--------------------------------------------------------------------------\n",
      "Input size (MB): 0.011719\n",
      "Forward/backward pass size (MB): 0.359634\n",
      "Params size (MB): 0.207035\n",
      "Estimated Total Size (MB): 0.578388\n",
      "--------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'--------------------------------------------------------------------------\\nLayer (type)                            Output Shape              Param #\\n==========================================================================\\nConv2d-1                            [-1, 32, 30, 30]                  896\\nMaxPool2d-2                         [-1, 32, 15, 15]                    0\\nConv2d-3                            [-1, 64, 11, 11]               51,264\\nMaxPool2d-4                           [-1, 64, 5, 5]                    0\\nDropout2d-5                           [-1, 64, 5, 5]                    0\\nAdaptiveMaxPool2d-6                   [-1, 64, 1, 1]                    0\\nFlatten-7                                   [-1, 64]                    0\\nLinear-8                                    [-1, 32]                2,080\\nReLU-9                                      [-1, 32]                    0\\nLinear-10                                    [-1, 1]                   33\\nSigmoid-11                                   [-1, 1]                    0\\n==========================================================================\\nTotal params: 54,273\\nTrainable params: 54,273\\nNon-trainable params: 0\\n--------------------------------------------------------------------------\\nInput size (MB): 0.011719\\nForward/backward pass size (MB): 0.359634\\nParams size (MB): 0.207035\\nEstimated Total Size (MB): 0.578388\\n--------------------------------------------------------------------------'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 继承nn.Module基类构建自定义模型\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = 2,stride = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = 2,stride = 2)\n",
    "        self.dropout = nn.Dropout2d(p = 0.1)\n",
    "        self.adaptive_pool = nn.AdaptiveMaxPool2d((1,1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(64,32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(32,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        y = self.sigmoid(x)\n",
    "        return y \n",
    "      \n",
    "net = Net()\n",
    "print(net)\n",
    "summary(net, input_shape=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcd3e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 使用nn.Sequential按层顺序构建模型(无需定义forward方法)\n",
    "\n",
    "# 1.利用add_module方法\n",
    "net = nn.Sequential()\n",
    "net.add_module(\"conv1\",nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3))\n",
    "net.add_module(\"pool1\",nn.MaxPool2d(kernel_size = 2,stride = 2))\n",
    "net.add_module(\"conv2\",nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5))\n",
    "net.add_module(\"pool2\",nn.MaxPool2d(kernel_size = 2,stride = 2))\n",
    "net.add_module(\"dropout\",nn.Dropout2d(p = 0.1))\n",
    "net.add_module(\"adaptive_pool\",nn.AdaptiveMaxPool2d((1,1)))\n",
    "net.add_module(\"flatten\",nn.Flatten())\n",
    "net.add_module(\"linear1\",nn.Linear(64,32))\n",
    "net.add_module(\"relu\",nn.ReLU())\n",
    "net.add_module(\"linear2\",nn.Linear(32,1))\n",
    "net.add_module(\"sigmoid\",nn.Sigmoid())\n",
    "# print(net)\n",
    "\n",
    "# 2.利用变长参数\n",
    "net1 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3),\n",
    "    nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "    nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),\n",
    "    nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "    nn.Dropout2d(p = 0.1),\n",
    "    nn.AdaptiveMaxPool2d((1,1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64,32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32,1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "# print(net1)\n",
    "\n",
    "# 3.利用OrderedDict\n",
    "from collections import OrderedDict\n",
    "net2 = nn.Sequential(OrderedDict(\n",
    "          [(\"conv1\",nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3)),\n",
    "            (\"pool1\",nn.MaxPool2d(kernel_size = 2,stride = 2)),\n",
    "            (\"conv2\",nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5)),\n",
    "            (\"pool2\",nn.MaxPool2d(kernel_size = 2,stride = 2)),\n",
    "            (\"dropout\",nn.Dropout2d(p = 0.1)),\n",
    "            (\"adaptive_pool\",nn.AdaptiveMaxPool2d((1,1))),\n",
    "            (\"flatten\",nn.Flatten()),\n",
    "            (\"linear1\",nn.Linear(64,32)),\n",
    "            (\"relu\",nn.ReLU()),\n",
    "            (\"linear2\",nn.Linear(32,1)),\n",
    "            (\"sigmoid\",nn.Sigmoid())\n",
    "          ])\n",
    "        )\n",
    "# print(net2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c99d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 继承nn.Module基类构建模型并辅助应用模型容器进行封装\n",
    "\n",
    "# 1.nn.Sequential作为模型容器\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3),\n",
    "            nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "            nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),\n",
    "            nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "            nn.Dropout2d(p = 0.1),\n",
    "            nn.AdaptiveMaxPool2d((1,1))\n",
    "        )\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        y = self.dense(x)\n",
    "        return y \n",
    "# net = Net()\n",
    "# print(net)\n",
    "\n",
    "# 2.nn.ModuleList作为模型容器\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3),\n",
    "            nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "            nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),\n",
    "            nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "            nn.Dropout2d(p = 0.1),\n",
    "            nn.AdaptiveMaxPool2d((1,1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,1),\n",
    "            nn.Sigmoid()]\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "# net = Net()\n",
    "# print(net)\n",
    "\n",
    "# 3.nn.ModuleDict作为模型容器\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers_dict = nn.ModuleDict({\"conv1\":nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3),\n",
    "               \"pool\": nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "               \"conv2\":nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),\n",
    "               \"dropout\": nn.Dropout2d(p = 0.1),\n",
    "               \"adaptive\":nn.AdaptiveMaxPool2d((1,1)),\n",
    "               \"flatten\": nn.Flatten(),\n",
    "               \"linear1\": nn.Linear(64,32),\n",
    "               \"relu\":nn.ReLU(),\n",
    "               \"linear2\": nn.Linear(32,1),\n",
    "               \"sigmoid\": nn.Sigmoid()\n",
    "              })\n",
    "    def forward(self,x):\n",
    "        layers = [\"conv1\",\"pool\",\"conv2\",\"pool\",\"dropout\",\"adaptive\",\n",
    "                  \"flatten\",\"linear1\",\"relu\",\"linear2\",\"sigmoid\"]\n",
    "        for layer in layers:\n",
    "            x = self.layers_dict[layer](x)\n",
    "        return x\n",
    "# net = Net()\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ad5606",
   "metadata": {},
   "source": [
    "## 保存与加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9637d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 保存方法 1 \n",
    "# torch.save(vgg16,\"vgg16_method1.pth\") # 保存结构模型和参数\n",
    "# # 加载模型 1\n",
    "# model = torch.load(\"vgg16_method1.pth\")\n",
    "\n",
    "# # 保存方式 2 -- 以字典方式只保存参数（官方推荐),\n",
    "# torch.save(vgg16.state_dict(),\"vgg_method2.pth\") \n",
    "# # 加载方式 2 -- 要恢复网络模型\n",
    "# model = torch.load(\"vgg_method2.pth\")\n",
    "# vgg16 = torchvision.models.vgg16(pretrained = True)\n",
    "# vgg16.load_state_dict(torch.load(\"vgg_method2.pth\"))\n",
    "\n",
    "\n",
    "# # 释放显卡的缓存，会减慢速度\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c0c81",
   "metadata": {},
   "source": [
    "## 梯度和求导\n",
    "[一文解释 PyTorch求导相关 (backward, autograd.grad)](https://zhuanlan.zhihu.com/p/279758736)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4c44072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====backward=====\n",
      "requires_grad:  True True True True\n",
      "is_leaf:  True False False False\n",
      "grad:  tensor(7.) None None None\n",
      "=====autograd.grad=====\n",
      "requires_grad:  True True True True\n",
      "is_leaf:  True False False False\n",
      "grad:  None None None None\n",
      "(tensor(7.),)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Download\\Miniconda\\envs\\pt1.12\\lib\\site-packages\\torch\\_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:482.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "import torch    \n",
    "\n",
    "x = torch.tensor(2. ,requires_grad=True)\n",
    "a = torch.add(x,1)\n",
    "b = torch.add(x,2)\n",
    "y = torch.mul(a,b)\n",
    "y.backward()\n",
    "print(\"=====backward=====\")\n",
    "print(\"requires_grad: \", x.requires_grad, a.requires_grad, b.requires_grad, y.requires_grad)\n",
    "print(\"is_leaf: \", x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)\n",
    "print(\"grad: \", x.grad, a.grad, b.grad, y.grad)\n",
    "\n",
    "x = torch.tensor(2., requires_grad=True)\n",
    "a = torch.add(x, 1)\n",
    "b = torch.add(x, 2)\n",
    "y = torch.mul(a, b)\n",
    "grad = torch.autograd.grad(outputs=y, inputs=x)\n",
    "print(\"=====autograd.grad=====\")\n",
    "print(\"requires_grad: \", x.requires_grad, a.requires_grad, b.requires_grad, y.requires_grad)\n",
    "print(\"is_leaf: \", x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)\n",
    "print(\"grad: \", x.grad, a.grad, b.grad, y.grad)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99911a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====backward=====\n",
      "tensor(12.) tensor(4.)\n",
      "=====autograd.grad=====\n",
      "(tensor(12.),)\n",
      "=====保留计算图，求偏导=====\n",
      "(tensor(12.),) (tensor(4.),)\n",
      "=====autograd.grad,二阶求导=====\n",
      "(tensor(12., grad_fn=<AddBackward0>),) (tensor(6.),)\n",
      "=====autograd.grad() + backward(),二阶求导=====\n",
      "tensor(6.) tensor(4.) None None\n",
      "=====backward() + autograd.grad(),二阶求导=====\n",
      "(tensor(6.),) tensor(12., grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "print(\"=====backward=====\")\n",
    "x = torch.tensor(2., requires_grad=True)\n",
    "y = torch.tensor(3., requires_grad=True)\n",
    "z = x * x * y\n",
    "z.backward()\n",
    "print(x.grad, y.grad)\n",
    "\n",
    "print(\"=====autograd.grad=====\")\n",
    "x = torch.tensor(2., requires_grad=True)\n",
    "y = torch.tensor(3., requires_grad=True)\n",
    "z = x * x * y\n",
    "grad_x = torch.autograd.grad(outputs=z, inputs=x)\n",
    "print(grad_x)\n",
    "\n",
    "print(\"=====保留计算图，求偏导=====\")\n",
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "z = x * x * y\n",
    "grad_x = torch.autograd.grad(outputs=z, inputs=x, retain_graph=True) # 保留计算图和梯度\n",
    "grad_y = torch.autograd.grad(outputs=z, inputs=y)\n",
    "print(grad_x, grad_y)\n",
    "\n",
    "\n",
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "z = x * x * y\n",
    "print(\"=====autograd.grad,二阶求导=====\")\n",
    "grad_x = torch.autograd.grad(outputs=z, inputs=x, create_graph=True) # 保留原图的基础上创建新图\n",
    "grad_xx = torch.autograd.grad(outputs=grad_x, inputs=x)\n",
    "print(grad_x, grad_xx)\n",
    "\n",
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "z = x * x * y\n",
    "print(\"=====autograd.grad() + backward(),二阶求导=====\")\n",
    "grad = torch.autograd.grad(outputs=z, inputs=[x, y], create_graph=True)\n",
    "grad[0].backward()\n",
    "print(x.grad,y.grad,grad[0].grad,grad[1].grad)\n",
    "\n",
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "z = x * x * y\n",
    "print(\"=====backward() + autograd.grad(),二阶求导=====\")\n",
    "z.backward(create_graph=True)\n",
    "grad_xx = torch.autograd.grad(outputs=x.grad, inputs=x)\n",
    "print(grad_xx, x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de038c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====backward() + backward(),二阶求导=====\n",
      "tensor(18., grad_fn=<CopyBackwards>)\n",
      "=====梯度清零,二阶求导=====\n",
      "tensor(6., grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "# 梯度清零\n",
    "print(\"=====backward() + backward(),二阶求导=====\")\n",
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "z = x * x * y\n",
    "z.backward(create_graph=True) # x.gtad = dz/dx = 12\n",
    "x.grad.backward() # 二阶：d(2xy)/dx = 2y=6 6+12=18\n",
    "print(x.grad)\n",
    "\n",
    "print(\"=====梯度清零,二阶求导=====\")\n",
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "z = x * x * y\n",
    "z.backward(create_graph=True)\n",
    "x.grad.data.zero_() # 梯度清零\n",
    "x.grad.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dae5d3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4.])\n",
      "=====求导计算的雅可比矩阵=====\n",
      "tensor([2., 4.])\n"
     ]
    }
   ],
   "source": [
    "# 向量求导: 只能标量对标量，标量对向量求梯度\n",
    "x = torch.tensor([1., 2.]).requires_grad_()\n",
    "y = x * x\n",
    "y.sum().backward() # y.sum() = x1^2 + x2^2,sum对求偏导没有影响\n",
    "# grad_x = torch.autograd.grad(outputs=y.sum(), inputs=x)\n",
    "print(x.grad)\n",
    "\n",
    "print(\"=====求导计算的雅可比矩阵=====\")\n",
    "x = torch.tensor([1., 2.]).requires_grad_()\n",
    "y = x * x\n",
    "y.backward(gradient=torch.ones_like(y))\n",
    "# grad_x = torch.autograd.grad(outputs=y, inputs=x, grad_outputs=torch.ones_like(y))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "873a26b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requires_grad:  True False True True\n",
      "is_leaf:  True True False False\n",
      "grad:  tensor([3.]) None None None\n"
     ]
    }
   ],
   "source": [
    "# 使用detach()切断\n",
    "x = torch.tensor([2.] ,requires_grad=True)\n",
    "a = torch.add(x,1).detach()\n",
    "b = torch.add(x,2)\n",
    "y = torch.mul(a,b)\n",
    "y.backward() # dy/dx = dy/da * da/dx + dy/db * db/dx\n",
    "print(\"requires_grad: \", x.requires_grad, a.requires_grad, b.requires_grad, y.requires_grad)\n",
    "print(\"is_leaf: \", x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)\n",
    "print(\"grad: \", x.grad, a.grad, b.grad, y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b334102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "bfd284e930bc93997eea298487d531cdfdbc0f39007072028ce7af8882c61801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
