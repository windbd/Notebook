{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "- huggingface下transformers库的分词工具：Tokenizer快速使用\n",
    "- 安装torchtext时需要考虑对应版本的问题：\n",
    "  - pytroch的的版本为1.a.b，则torchtext的版本为0.(a+1).b\n",
    "  - 若pytorch == 1.13.1，则pip install torchtext == 0.14.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "embed = torch.nn.Embedding(num_embeddings=10, embedding_dim=4)#[10,4]\n",
    "# word2id的作用，1表示EOS，2表示PAD\n",
    "batch = [[3, 6, 5, 6, 7, 1], [6, 4, 7, 9, 5, 1], [4, 5, 8, 7, 1, 2]]#[3,6]\n",
    "batch = torch.LongTensor(batch)\n",
    "batch = batch.reshape(6, 3)  # [seq_len,batch_size]排序>>为了在RNN模型中进行训练\n",
    "batch_embed = embed(batch)  # [6,3,10][10,4]=[6,3,4]，不能超过10类\n",
    "batch_embed.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV\n",
    "- Pytorch Image Models (timm)有常用的视觉模型：Pytorch视觉模型库--timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加载CV领域的模型结构（方法一）\n",
    "\n",
    "# 参考链接：https://blog.csdn.net/me_yundou/article/details/109218273\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# resnet = torchvision.models.resnet50(pretrained=False)#无预训练参数\n",
    "model = torchvision.models.vgg19(pretrained=True)\n",
    "print(model)#打印网络结构\n",
    "\n",
    "# model.classifier.add_module(\"add_linear\",nn.Linear(1000,10)) # 在resnet50的classfier里加一层\n",
    "# model.classifier[6] = nn.Linear(4096,10) # 修改对应层,编号相对应\n",
    "\n",
    "# model=list(model.children())[:-1]#去掉后一层只保留（2048/512，7，7）\n",
    "# model = torch.nn.Sequential(*model)\n",
    "\n",
    "# input = torch.randn(2,3,224,224)\n",
    "# output = model(input)\n",
    "# output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加载CVCV领域的模型结构（方法二）\n",
    "\n",
    "import timm\n",
    "model = timm.create_model('vgg19',pretrained=True)\n",
    "# model = timm.create_model(\"hf_hub:timm/vgg19.tv_in1k\", pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch\n",
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchkeras import summary\n",
    "\n",
    "## 继承nn.Module基类构建自定义模型\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = 2,stride = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = 2,stride = 2)\n",
    "        self.dropout = nn.Dropout2d(p = 0.1)\n",
    "        self.adaptive_pool = nn.AdaptiveMaxPool2d((1,1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(64,32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(32,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        y = self.sigmoid(x)\n",
    "        return y \n",
    "      \n",
    "net = Net()\n",
    "print(net)\n",
    "summary(net, input_shape=(3, 32, 32))\n",
    "\n",
    "## 使用nn.Sequential按层顺序构建模型(无需定义forward方法)\n",
    "\n",
    "# 1.利用add_module方法\n",
    "net = nn.Sequential()\n",
    "net.add_module(\"conv1\",nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3))\n",
    "net.add_module(\"pool1\",nn.MaxPool2d(kernel_size = 2,stride = 2))\n",
    "net.add_module(\"conv2\",nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5))\n",
    "net.add_module(\"pool2\",nn.MaxPool2d(kernel_size = 2,stride = 2))\n",
    "net.add_module(\"dropout\",nn.Dropout2d(p = 0.1))\n",
    "net.add_module(\"adaptive_pool\",nn.AdaptiveMaxPool2d((1,1)))\n",
    "net.add_module(\"flatten\",nn.Flatten())\n",
    "net.add_module(\"linear1\",nn.Linear(64,32))\n",
    "net.add_module(\"relu\",nn.ReLU())\n",
    "net.add_module(\"linear2\",nn.Linear(32,1))\n",
    "net.add_module(\"sigmoid\",nn.Sigmoid())\n",
    "# print(net)\n",
    "\n",
    "# 2.利用变长参数\n",
    "net1 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3),\n",
    "    nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "    nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),\n",
    "    nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "    nn.Dropout2d(p = 0.1),\n",
    "    nn.AdaptiveMaxPool2d((1,1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64,32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32,1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "# print(net1)\n",
    "\n",
    "# 3.利用OrderedDict\n",
    "from collections import OrderedDict\n",
    "net2 = nn.Sequential(OrderedDict(\n",
    "          [(\"conv1\",nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3)),\n",
    "            (\"pool1\",nn.MaxPool2d(kernel_size = 2,stride = 2)),\n",
    "            (\"conv2\",nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5)),\n",
    "            (\"pool2\",nn.MaxPool2d(kernel_size = 2,stride = 2)),\n",
    "            (\"dropout\",nn.Dropout2d(p = 0.1)),\n",
    "            (\"adaptive_pool\",nn.AdaptiveMaxPool2d((1,1))),\n",
    "            (\"flatten\",nn.Flatten()),\n",
    "            (\"linear1\",nn.Linear(64,32)),\n",
    "            (\"relu\",nn.ReLU()),\n",
    "            (\"linear2\",nn.Linear(32,1)),\n",
    "            (\"sigmoid\",nn.Sigmoid())\n",
    "          ])\n",
    "        )\n",
    "# print(net2)\n",
    "\n",
    "## 继承nn.Module基类构建模型并辅助应用模型容器进行封装\n",
    "\n",
    "# 1.nn.Sequential作为模型容器\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3),\n",
    "            nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "            nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),\n",
    "            nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "            nn.Dropout2d(p = 0.1),\n",
    "            nn.AdaptiveMaxPool2d((1,1))\n",
    "        )\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        y = self.dense(x)\n",
    "        return y \n",
    "# net = Net()\n",
    "# print(net)\n",
    "\n",
    "# 2.nn.ModuleList作为模型容器\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3),\n",
    "            nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "            nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),\n",
    "            nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "            nn.Dropout2d(p = 0.1),\n",
    "            nn.AdaptiveMaxPool2d((1,1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,1),\n",
    "            nn.Sigmoid()]\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "# net = Net()\n",
    "# print(net)\n",
    "\n",
    "# 3.nn.ModuleDict作为模型容器\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers_dict = nn.ModuleDict({\"conv1\":nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3),\n",
    "               \"pool\": nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "               \"conv2\":nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),\n",
    "               \"dropout\": nn.Dropout2d(p = 0.1),\n",
    "               \"adaptive\":nn.AdaptiveMaxPool2d((1,1)),\n",
    "               \"flatten\": nn.Flatten(),\n",
    "               \"linear1\": nn.Linear(64,32),\n",
    "               \"relu\":nn.ReLU(),\n",
    "               \"linear2\": nn.Linear(32,1),\n",
    "               \"sigmoid\": nn.Sigmoid()\n",
    "              })\n",
    "    def forward(self,x):\n",
    "        layers = [\"conv1\",\"pool\",\"conv2\",\"pool\",\"dropout\",\"adaptive\",\n",
    "                  \"flatten\",\"linear1\",\"relu\",\"linear2\",\"sigmoid\"]\n",
    "        for layer in layers:\n",
    "            x = self.layers_dict[layer](x)\n",
    "        return x\n",
    "# net = Net()\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度和求导\n",
    "[一文解释 PyTorch求导相关 (backward, autograd.grad)](https://zhuanlan.zhihu.com/p/279758736)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch    \n",
    "\n",
    "x = torch.tensor(2. ,requires_grad=True)\n",
    "a = torch.add(x,1)\n",
    "b = torch.add(x,2)\n",
    "y = torch.mul(a,b)\n",
    "y.backward()\n",
    "print(\"=====backward=====\")\n",
    "print(\"requires_grad: \", x.requires_grad, a.requires_grad, b.requires_grad, y.requires_grad)\n",
    "print(\"is_leaf: \", x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)\n",
    "print(\"grad: \", x.grad, a.grad, b.grad, y.grad)\n",
    "\n",
    "x = torch.tensor(2., requires_grad=True)\n",
    "a = torch.add(x, 1)\n",
    "b = torch.add(x, 2)\n",
    "y = torch.mul(a, b)\n",
    "grad = torch.autograd.grad(outputs=y, inputs=x)\n",
    "print(\"=====autograd.grad=====\")\n",
    "print(\"requires_grad: \", x.requires_grad, a.requires_grad, b.requires_grad, y.requires_grad)\n",
    "print(\"is_leaf: \", x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)\n",
    "print(\"grad: \", x.grad, a.grad, b.grad, y.grad)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=====backward=====\")\n",
    "x = torch.tensor(2., requires_grad=True)\n",
    "y = torch.tensor(3., requires_grad=True)\n",
    "z = x * x * y\n",
    "z.backward()\n",
    "print(x.grad, y.grad)\n",
    "\n",
    "print(\"=====autograd.grad=====\")\n",
    "x = torch.tensor(2., requires_grad=True)\n",
    "y = torch.tensor(3., requires_grad=True)\n",
    "z = x * x * y\n",
    "grad_x = torch.autograd.grad(outputs=z, inputs=x)\n",
    "print(grad_x)\n",
    "\n",
    "print(\"=====保留计算图，求偏导=====\")\n",
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "z = x * x * y\n",
    "grad_x = torch.autograd.grad(outputs=z, inputs=x, retain_graph=True) # 保留计算图和梯度\n",
    "grad_y = torch.autograd.grad(outputs=z, inputs=y)\n",
    "print(grad_x, grad_y)\n",
    "\n",
    "\n",
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "z = x * x * y\n",
    "print(\"=====autograd.grad,二阶求导=====\")\n",
    "grad_x = torch.autograd.grad(outputs=z, inputs=x, create_graph=True) # 保留原图的基础上创建新图\n",
    "grad_xx = torch.autograd.grad(outputs=grad_x, inputs=x)\n",
    "print(grad_x, grad_xx)\n",
    "\n",
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "z = x * x * y\n",
    "print(\"=====autograd.grad() + backward(),二阶求导=====\")\n",
    "grad = torch.autograd.grad(outputs=z, inputs=[x, y], create_graph=True)\n",
    "grad[0].backward()\n",
    "print(x.grad,y.grad,grad[0].grad,grad[1].grad)\n",
    "\n",
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "z = x * x * y\n",
    "print(\"=====backward() + autograd.grad(),二阶求导=====\")\n",
    "z.backward(create_graph=True)\n",
    "grad_xx = torch.autograd.grad(outputs=x.grad, inputs=x)\n",
    "print(grad_xx, x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度清零\n",
    "print(\"=====backward() + backward(),二阶求导=====\")\n",
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "z = x * x * y\n",
    "z.backward(create_graph=True) # x.gtad = dz/dx = 12\n",
    "x.grad.backward() # 二阶：d(2xy)/dx = 2y=6 6+12=18\n",
    "print(x.grad)\n",
    "\n",
    "print(\"=====梯度清零,二阶求导=====\")\n",
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "z = x * x * y\n",
    "z.backward(create_graph=True)\n",
    "x.grad.data.zero_() # 梯度清零\n",
    "x.grad.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量求导: 只能标量对标量，标量对向量求梯度\n",
    "x = torch.tensor([1., 2.]).requires_grad_()\n",
    "y = x * x\n",
    "y.sum().backward() # y.sum() = x1^2 + x2^2,sum对求偏导没有影响\n",
    "# grad_x = torch.autograd.grad(outputs=y.sum(), inputs=x)\n",
    "print(x.grad)\n",
    "\n",
    "print(\"=====求导计算的雅可比矩阵=====\")\n",
    "x = torch.tensor([1., 2.]).requires_grad_()\n",
    "y = x * x\n",
    "y.backward(gradient=torch.ones_like(y))\n",
    "# grad_x = torch.autograd.grad(outputs=y, inputs=x, grad_outputs=torch.ones_like(y))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用detach()切断\n",
    "x = torch.tensor([2.] ,requires_grad=True)\n",
    "a = torch.add(x,1).detach()\n",
    "b = torch.add(x,2)\n",
    "y = torch.mul(a,b)\n",
    "y.backward() # dy/dx = dy/da * da/dx + dy/db * db/dx\n",
    "print(\"requires_grad: \", x.requires_grad, a.requires_grad, b.requires_grad, y.requires_grad)\n",
    "print(\"is_leaf: \", x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)\n",
    "print(\"grad: \", x.grad, a.grad, b.grad, y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vit_pytorch\n",
    "- https://github.com/lucidrains/vit-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "v = ViT(\n",
    "    image_size = 256,\n",
    "    patch_size = 32,\n",
    "    num_classes = 1000,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "\n",
    "img = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "preds = v(img) # (1, 1000)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wandb  \n",
    "- https://wandb.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(settings=wandb.Settings(start_method=\"thread\"))\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# simulate training\n",
    "epochs = 10\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "    loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "    \n",
    "    # log metrics to wandb\n",
    "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "    \n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGluon \n",
    "- https://auto.gluon.ai/stable/index.html\n",
    "- Tabular\n",
    "- Multimodal\n",
    "- Time Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "data_root = '../data/autogluon/'\n",
    "train_data = TabularDataset(data_root + 'train.csv')\n",
    "test_data = TabularDataset(data_root + 'test.csv')\n",
    "\n",
    "predictor = TabularPredictor(label='class').fit(train_data=train_data)# hyperparameters='multimodal',num_stack_levels=1, num_bagging_folds=5\n",
    "predictions = predictor.predict(data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
