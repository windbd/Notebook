{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "- huggingface下transformers库的分词工具：Tokenizer快速使用\n",
    "- 安装torchtext时需要考虑对应版本的问题：\n",
    "  - pytroch的的版本为1.a.b，则torchtext的版本为0.(a+1).b\n",
    "  - 若pytorch == 1.13.1，则pip install torchtext == 0.14.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "embed = torch.nn.Embedding(num_embeddings=10, embedding_dim=4)#[10,4]\n",
    "# word2id的作用，1表示EOS，2表示PAD\n",
    "batch = [[3, 6, 5, 6, 7, 1], [6, 4, 7, 9, 5, 1], [4, 5, 8, 7, 1, 2]]#[3,6]\n",
    "batch = torch.LongTensor(batch)\n",
    "batch = batch.reshape(6, 3)  # [seq_len,batch_size]排序>>为了在RNN模型中进行训练\n",
    "batch_embed = embed(batch)  # [6,3,10][10,4]=[6,3,4]，不能超过10类\n",
    "batch_embed.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV\n",
    "- Pytorch Image Models (timm)有常用的视觉模型：Pytorch视觉模型库--timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加载CV领域的模型结构（方法一）\n",
    "\n",
    "# 参考链接：https://blog.csdn.net/me_yundou/article/details/109218273\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# resnet = torchvision.models.resnet50(pretrained=False)#无预训练参数\n",
    "model = torchvision.models.vgg19(pretrained=True)\n",
    "print(model)#打印网络结构\n",
    "\n",
    "# model.classifier.add_module(\"add_linear\",nn.Linear(1000,10)) # 在resnet50的classfier里加一层\n",
    "# model.classifier[6] = nn.Linear(4096,10) # 修改对应层,编号相对应\n",
    "\n",
    "# model=list(model.children())[:-1]#去掉后一层只保留（2048/512，7，7）\n",
    "# model = torch.nn.Sequential(*model)\n",
    "\n",
    "# input = torch.randn(2,3,224,224)\n",
    "# output = model(input)\n",
    "# output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加载CVCV领域的模型结构（方法二）\n",
    "\n",
    "import timm\n",
    "model = timm.create_model('vgg19',pretrained=True)\n",
    "# model = timm.create_model(\"hf_hub:timm/vgg19.tv_in1k\", pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch\n",
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchkeras import summary\n",
    "\n",
    "## 继承nn.Module基类构建自定义模型\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = 2,stride = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = 2,stride = 2)\n",
    "        self.dropout = nn.Dropout2d(p = 0.1)\n",
    "        self.adaptive_pool = nn.AdaptiveMaxPool2d((1,1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(64,32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(32,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        y = self.sigmoid(x)\n",
    "        return y \n",
    "      \n",
    "net = Net()\n",
    "print(net)\n",
    "summary(net, input_shape=(3, 32, 32))\n",
    "\n",
    "## 使用nn.Sequential按层顺序构建模型(无需定义forward方法)\n",
    "\n",
    "# 1.利用add_module方法\n",
    "net = nn.Sequential()\n",
    "net.add_module(\"conv1\",nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3))\n",
    "net.add_module(\"pool1\",nn.MaxPool2d(kernel_size = 2,stride = 2))\n",
    "net.add_module(\"conv2\",nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5))\n",
    "net.add_module(\"pool2\",nn.MaxPool2d(kernel_size = 2,stride = 2))\n",
    "net.add_module(\"dropout\",nn.Dropout2d(p = 0.1))\n",
    "net.add_module(\"adaptive_pool\",nn.AdaptiveMaxPool2d((1,1)))\n",
    "net.add_module(\"flatten\",nn.Flatten())\n",
    "net.add_module(\"linear1\",nn.Linear(64,32))\n",
    "net.add_module(\"relu\",nn.ReLU())\n",
    "net.add_module(\"linear2\",nn.Linear(32,1))\n",
    "net.add_module(\"sigmoid\",nn.Sigmoid())\n",
    "# print(net)\n",
    "\n",
    "# 2.利用变长参数\n",
    "net1 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3),\n",
    "    nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "    nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),\n",
    "    nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "    nn.Dropout2d(p = 0.1),\n",
    "    nn.AdaptiveMaxPool2d((1,1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64,32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32,1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "# print(net1)\n",
    "\n",
    "# 3.利用OrderedDict\n",
    "from collections import OrderedDict\n",
    "net2 = nn.Sequential(OrderedDict(\n",
    "          [(\"conv1\",nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3)),\n",
    "            (\"pool1\",nn.MaxPool2d(kernel_size = 2,stride = 2)),\n",
    "            (\"conv2\",nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5)),\n",
    "            (\"pool2\",nn.MaxPool2d(kernel_size = 2,stride = 2)),\n",
    "            (\"dropout\",nn.Dropout2d(p = 0.1)),\n",
    "            (\"adaptive_pool\",nn.AdaptiveMaxPool2d((1,1))),\n",
    "            (\"flatten\",nn.Flatten()),\n",
    "            (\"linear1\",nn.Linear(64,32)),\n",
    "            (\"relu\",nn.ReLU()),\n",
    "            (\"linear2\",nn.Linear(32,1)),\n",
    "            (\"sigmoid\",nn.Sigmoid())\n",
    "          ])\n",
    "        )\n",
    "# print(net2)\n",
    "\n",
    "## 继承nn.Module基类构建模型并辅助应用模型容器进行封装\n",
    "\n",
    "# 1.nn.Sequential作为模型容器\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3),\n",
    "            nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "            nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),\n",
    "            nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "            nn.Dropout2d(p = 0.1),\n",
    "            nn.AdaptiveMaxPool2d((1,1))\n",
    "        )\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        y = self.dense(x)\n",
    "        return y \n",
    "# net = Net()\n",
    "# print(net)\n",
    "\n",
    "# 2.nn.ModuleList作为模型容器\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3),\n",
    "            nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "            nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),\n",
    "            nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "            nn.Dropout2d(p = 0.1),\n",
    "            nn.AdaptiveMaxPool2d((1,1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,1),\n",
    "            nn.Sigmoid()]\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "# net = Net()\n",
    "# print(net)\n",
    "\n",
    "# 3.nn.ModuleDict作为模型容器\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers_dict = nn.ModuleDict({\"conv1\":nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3),\n",
    "               \"pool\": nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "               \"conv2\":nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),\n",
    "               \"dropout\": nn.Dropout2d(p = 0.1),\n",
    "               \"adaptive\":nn.AdaptiveMaxPool2d((1,1)),\n",
    "               \"flatten\": nn.Flatten(),\n",
    "               \"linear1\": nn.Linear(64,32),\n",
    "               \"relu\":nn.ReLU(),\n",
    "               \"linear2\": nn.Linear(32,1),\n",
    "               \"sigmoid\": nn.Sigmoid()\n",
    "              })\n",
    "    def forward(self,x):\n",
    "        layers = [\"conv1\",\"pool\",\"conv2\",\"pool\",\"dropout\",\"adaptive\",\n",
    "                  \"flatten\",\"linear1\",\"relu\",\"linear2\",\"sigmoid\"]\n",
    "        for layer in layers:\n",
    "            x = self.layers_dict[layer](x)\n",
    "        return x\n",
    "# net = Net()\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度和求导\n",
    "[一文解释 PyTorch求导相关 (backward, autograd.grad)](https://zhuanlan.zhihu.com/p/279758736)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch    \n",
    "\n",
    "x = torch.tensor(2. ,requires_grad=True)\n",
    "a = torch.add(x,1)\n",
    "b = torch.add(x,2)\n",
    "y = torch.mul(a,b)\n",
    "y.backward()\n",
    "print(\"=====backward=====\")\n",
    "print(\"requires_grad: \", x.requires_grad, a.requires_grad, b.requires_grad, y.requires_grad)\n",
    "print(\"is_leaf: \", x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)\n",
    "print(\"grad: \", x.grad, a.grad, b.grad, y.grad)\n",
    "\n",
    "x = torch.tensor(2., requires_grad=True)\n",
    "a = torch.add(x, 1)\n",
    "b = torch.add(x, 2)\n",
    "y = torch.mul(a, b)\n",
    "grad = torch.autograd.grad(outputs=y, inputs=x)\n",
    "print(\"=====autograd.grad=====\")\n",
    "print(\"requires_grad: \", x.requires_grad, a.requires_grad, b.requires_grad, y.requires_grad)\n",
    "print(\"is_leaf: \", x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)\n",
    "print(\"grad: \", x.grad, a.grad, b.grad, y.grad)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=====backward=====\")\n",
    "x = torch.tensor(2., requires_grad=True)\n",
    "y = torch.tensor(3., requires_grad=True)\n",
    "z = x * x * y\n",
    "z.backward()\n",
    "print(x.grad, y.grad)\n",
    "\n",
    "print(\"=====autograd.grad=====\")\n",
    "x = torch.tensor(2., requires_grad=True)\n",
    "y = torch.tensor(3., requires_grad=True)\n",
    "z = x * x * y\n",
    "grad_x = torch.autograd.grad(outputs=z, inputs=x)\n",
    "print(grad_x)\n",
    "\n",
    "print(\"=====保留计算图，求偏导=====\")\n",
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "z = x * x * y\n",
    "grad_x = torch.autograd.grad(outputs=z, inputs=x, retain_graph=True) # 保留计算图和梯度\n",
    "grad_y = torch.autograd.grad(outputs=z, inputs=y)\n",
    "print(grad_x, grad_y)\n",
    "\n",
    "\n",
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "z = x * x * y\n",
    "print(\"=====autograd.grad,二阶求导=====\")\n",
    "grad_x = torch.autograd.grad(outputs=z, inputs=x, create_graph=True) # 保留原图的基础上创建新图\n",
    "grad_xx = torch.autograd.grad(outputs=grad_x, inputs=x)\n",
    "print(grad_x, grad_xx)\n",
    "\n",
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "z = x * x * y\n",
    "print(\"=====autograd.grad() + backward(),二阶求导=====\")\n",
    "grad = torch.autograd.grad(outputs=z, inputs=[x, y], create_graph=True)\n",
    "grad[0].backward()\n",
    "print(x.grad,y.grad,grad[0].grad,grad[1].grad)\n",
    "\n",
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "z = x * x * y\n",
    "print(\"=====backward() + autograd.grad(),二阶求导=====\")\n",
    "z.backward(create_graph=True)\n",
    "grad_xx = torch.autograd.grad(outputs=x.grad, inputs=x)\n",
    "print(grad_xx, x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度清零\n",
    "print(\"=====backward() + backward(),二阶求导=====\")\n",
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "z = x * x * y\n",
    "z.backward(create_graph=True) # x.gtad = dz/dx = 12\n",
    "x.grad.backward() # 二阶：d(2xy)/dx = 2y=6 6+12=18\n",
    "print(x.grad)\n",
    "\n",
    "print(\"=====梯度清零,二阶求导=====\")\n",
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "z = x * x * y\n",
    "z.backward(create_graph=True)\n",
    "x.grad.data.zero_() # 梯度清零\n",
    "x.grad.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量求导: 只能标量对标量，标量对向量求梯度\n",
    "x = torch.tensor([1., 2.]).requires_grad_()\n",
    "y = x * x\n",
    "y.sum().backward() # y.sum() = x1^2 + x2^2,sum对求偏导没有影响\n",
    "# grad_x = torch.autograd.grad(outputs=y.sum(), inputs=x)\n",
    "print(x.grad)\n",
    "\n",
    "print(\"=====求导计算的雅可比矩阵=====\")\n",
    "x = torch.tensor([1., 2.]).requires_grad_()\n",
    "y = x * x\n",
    "y.backward(gradient=torch.ones_like(y))\n",
    "# grad_x = torch.autograd.grad(outputs=y, inputs=x, grad_outputs=torch.ones_like(y))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用detach()切断\n",
    "x = torch.tensor([2.] ,requires_grad=True)\n",
    "a = torch.add(x,1).detach()\n",
    "b = torch.add(x,2)\n",
    "y = torch.mul(a,b)\n",
    "y.backward() # dy/dx = dy/da * da/dx + dy/db * db/dx\n",
    "print(\"requires_grad: \", x.requires_grad, a.requires_grad, b.requires_grad, y.requires_grad)\n",
    "print(\"is_leaf: \", x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)\n",
    "print(\"grad: \", x.grad, a.grad, b.grad, y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 0.958274442255497, Accuracy: 0.70453125\n",
      "Epoch 1, Batch 200, Loss: 0.4114549823105335, Accuracy: 0.87515625\n",
      "Epoch 1, Batch 300, Loss: 0.3639001946151257, Accuracy: 0.88921875\n",
      "Epoch 1, Batch 400, Loss: 0.33598140612244604, Accuracy: 0.89734375\n",
      "Epoch 1, Batch 500, Loss: 0.31051422879099844, Accuracy: 0.9078125\n",
      "Epoch 1, Batch 600, Loss: 0.31353990726172926, Accuracy: 0.90734375\n",
      "Epoch 1, Batch 700, Loss: 0.2756995743513107, Accuracy: 0.92171875\n",
      "Epoch 1, Batch 800, Loss: 0.2456675610691309, Accuracy: 0.92734375\n",
      "Epoch 1, Batch 900, Loss: 0.22653725825250148, Accuracy: 0.9325\n",
      "Epoch 2, Batch 100, Loss: 0.20561796076595784, Accuracy: 0.94078125\n",
      "Epoch 2, Batch 200, Loss: 0.20467623963952064, Accuracy: 0.94046875\n",
      "Epoch 2, Batch 300, Loss: 0.1883347226306796, Accuracy: 0.94546875\n",
      "Epoch 2, Batch 400, Loss: 0.1678795058093965, Accuracy: 0.95046875\n",
      "Epoch 2, Batch 500, Loss: 0.18480229552835226, Accuracy: 0.94546875\n",
      "Epoch 2, Batch 600, Loss: 0.17126287873834373, Accuracy: 0.95203125\n",
      "Epoch 2, Batch 700, Loss: 0.16673405427485705, Accuracy: 0.95046875\n",
      "Epoch 2, Batch 800, Loss: 0.16614491358399391, Accuracy: 0.95328125\n",
      "Epoch 2, Batch 900, Loss: 0.16339932881295682, Accuracy: 0.95078125\n",
      "Epoch 3, Batch 100, Loss: 0.14416173253208397, Accuracy: 0.9559375\n",
      "Epoch 3, Batch 200, Loss: 0.13703069807961582, Accuracy: 0.9553125\n",
      "Epoch 3, Batch 300, Loss: 0.15070080559700727, Accuracy: 0.954375\n",
      "Epoch 3, Batch 400, Loss: 0.12473198913037777, Accuracy: 0.96265625\n",
      "Epoch 3, Batch 500, Loss: 0.12869960658252239, Accuracy: 0.96125\n",
      "Epoch 3, Batch 600, Loss: 0.12405451348051429, Accuracy: 0.9634375\n",
      "Epoch 3, Batch 700, Loss: 0.13412081010639668, Accuracy: 0.96\n",
      "Epoch 3, Batch 800, Loss: 0.12632827246561645, Accuracy: 0.96015625\n",
      "Epoch 3, Batch 900, Loss: 0.12337434072047472, Accuracy: 0.96421875\n",
      "Epoch 4, Batch 100, Loss: 0.10777624131180347, Accuracy: 0.9665625\n",
      "Epoch 4, Batch 200, Loss: 0.10948997043073178, Accuracy: 0.96671875\n",
      "Epoch 4, Batch 300, Loss: 0.11030965524725617, Accuracy: 0.96828125\n",
      "Epoch 4, Batch 400, Loss: 0.09996992070227861, Accuracy: 0.97109375\n",
      "Epoch 4, Batch 500, Loss: 0.10655374318361283, Accuracy: 0.9690625\n",
      "Epoch 4, Batch 600, Loss: 0.10194947434589267, Accuracy: 0.9675\n",
      "Epoch 4, Batch 700, Loss: 0.10034855982288718, Accuracy: 0.96953125\n",
      "Epoch 4, Batch 800, Loss: 0.11701712775975466, Accuracy: 0.96625\n",
      "Epoch 4, Batch 900, Loss: 0.09401540402323008, Accuracy: 0.9709375\n",
      "Epoch 5, Batch 100, Loss: 0.08259222676977515, Accuracy: 0.9765625\n",
      "Epoch 5, Batch 200, Loss: 0.0945215437747538, Accuracy: 0.97\n",
      "Epoch 5, Batch 300, Loss: 0.09206369115039706, Accuracy: 0.97359375\n",
      "Epoch 5, Batch 400, Loss: 0.08860689790919424, Accuracy: 0.974375\n",
      "Epoch 5, Batch 500, Loss: 0.08249289448373019, Accuracy: 0.9759375\n",
      "Epoch 5, Batch 600, Loss: 0.08640723193064331, Accuracy: 0.9728125\n",
      "Epoch 5, Batch 700, Loss: 0.09457120528444647, Accuracy: 0.969375\n",
      "Epoch 5, Batch 800, Loss: 0.08411528329364955, Accuracy: 0.97546875\n",
      "Epoch 5, Batch 900, Loss: 0.08219290403649211, Accuracy: 0.97640625\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 初始化TensorBoard的SummaryWriter\n",
    "writer = SummaryWriter(log_dir='../data/tensorboard/',filename_suffix='simple_net')\n",
    "# 定义一个简单的神经网络\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 设置数据集和数据加载器\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='../data', train=True, download=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 初始化网络、损失函数和优化器\n",
    "model = SimpleNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "# 将网络结构写入TensorBoard\n",
    "writer.add_graph(model, next(iter(train_loader))[0])\n",
    "\n",
    "# 训练网络并记录损失\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    correct_num = 0\n",
    "    total_num = 0\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_num += labels.size(0)\n",
    "        correct_num += (predicted == labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # 每100个批次记录一次\n",
    "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}, Accuracy: {correct_num / total_num}')\n",
    "            writer.add_scalar('train/loss', running_loss / 100, epoch * len(train_loader) + i)\n",
    "            writer.add_scalar('train/accuracy', correct_num / total_num, epoch * len(train_loader) + i)\n",
    "            running_loss = 0.0\n",
    "            correct_num = 0\n",
    "            total_num = 0\n",
    "print('Finished Training')\n",
    "\n",
    "# 关闭SummaryWriter\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vit_pytorch\n",
    "- https://github.com/lucidrains/vit-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "v = ViT(\n",
    "    image_size = 256,\n",
    "    patch_size = 32,\n",
    "    num_classes = 1000,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "\n",
    "img = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "preds = v(img) # (1, 1000)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wandb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwindbd\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../data/wandb\\run-20241128_200522-yajh9ljv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/windbd/test-project/runs/yajh9ljv' target=\"_blank\">test-model</a></strong> to <a href='https://wandb.ai/windbd/test-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/windbd/test-project' target=\"_blank\">https://wandb.ai/windbd/test-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/windbd/test-project/runs/yajh9ljv' target=\"_blank\">https://wandb.ai/windbd/test-project/runs/yajh9ljv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e32f71e8990440eb67eb36a545f4e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.008 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▃▁▅▅▇▆▆▆▇██▇▇▇███▇</td></tr><tr><td>loss</td><td>██▃▂▃▂▂▂▁▂▂▂▁▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.87552</td></tr><tr><td>loss</td><td>0.11704</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test-model</strong> at: <a href='https://wandb.ai/windbd/test-project/runs/yajh9ljv' target=\"_blank\">https://wandb.ai/windbd/test-project/runs/yajh9ljv</a><br/> View project at: <a href='https://wandb.ai/windbd/test-project' target=\"_blank\">https://wandb.ai/windbd/test-project</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../data/wandb\\run-20241128_200522-yajh9ljv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# 设置超参数和配置\n",
    "config = {\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 20,\n",
    "}\n",
    "\n",
    "# 初始化 wandb 运行\n",
    "wandb.init(\n",
    "    dir=\"../data/\",\n",
    "    project=\"test-project\",\n",
    "    config=config, # 保存配置,方便后续查看\n",
    "    name=\"test-model\", \n",
    "    tags=[\"cifar\", \"baseline\", \"cnn\"], # 添加标签，方便筛选\n",
    "    mode=\"online\"\n",
    ")\n",
    "\n",
    "# 模拟训练\n",
    "epochs = config[\"epochs\"]\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "    loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "    \n",
    "    # 记录指标到 wandb\n",
    "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "    \n",
    "# 完成 wandb 运行\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGluon \n",
    "- https://auto.gluon.ai/stable/index.html\n",
    "- Tabular\n",
    "- Multimodal\n",
    "- Time Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "data_root = '../data/autogluon/'\n",
    "train_data = TabularDataset(data_root + 'train.csv')\n",
    "test_data = TabularDataset(data_root + 'test.csv')\n",
    "\n",
    "predictor = TabularPredictor(label='class').fit(train_data=train_data)# hyperparameters='multimodal',num_stack_levels=1, num_bagging_folds=5\n",
    "predictions = predictor.predict(data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
