{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图神经网络\n",
    "[图神经网络（GNN）最简单全面原理与代码实现](https://zhuanlan.zhihu.com/p/603486955)\n",
    "- 图数据是由节点（Node）和边（Edge）组成的数据\n",
    "- 图数据的信息包含3个层面，分别是**节点信息（V）、边信息（E）、图整体（U）**信息，对应着各自的任务\n",
    "- GNN是对图上的所有属性（V、E、U）进行的一个可以优化的变换\n",
    "- GNN对属性向量优化的方法叫做消息传递机制\n",
    "  - 比如最原始的GNN是SUM求和传递机制；\n",
    "  - 图卷积网络（GCN）考虑到了节点的度，度越大，权重越小，使用了加权的SUM；\n",
    "  - 图注意力网络GAT，在消息传递过程中引入了注意力机制；\n",
    "  - 不同GNN的本质差别就在于它们如何进行节点之间的信息传递和计算，即消息传递机制不同；\n",
    "- GNN对应任务的不同，在于output层的输入不同\n",
    "  - 对于节点层面的任务而言，可以直接self.conv = GCNConv(16, dataset.num_classes) \n",
    "  - 对于边层面的任务而言，通过GNN提取出节点信息，输入Output层之前需要进行边特征的融合\n",
    "  - 对于图层面的任务而言，通过GNN提取出节点信息，输入Output层之前需要进行图特征的融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载Cora数据集\n",
    "base_url = 'https://gitee.com/jiajiewu/planetoid/raw/master/data/ind.cora.'\n",
    "import requests\n",
    "names = ['x', 'tx', 'allx', 'y', 'ty', 'ally', 'graph', 'test.index']\n",
    "for name in names:\n",
    "    file_url=base_url+name\n",
    "    r=requests.get(file_url)\n",
    "    with open('../data/Cora/raw/ind.cora.'+name, 'wb') as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 节点分类任务代码实现\n",
    "学术论文的相关性分类问题（7个类别）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集准确率为：80.1%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "#载入数据\n",
    "dataset = Planetoid(root='../data', name='Cora')\n",
    "data = dataset[0] # x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708]\n",
    "#定义网络架构\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16)  #num_features=1433\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "    def forward(self, x, edge_index):\n",
    "        # x: [2708, 1433], edge_index: [2, 10556]\n",
    "        x = self.conv1(x, edge_index) # [2708, 16]\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index) # [2708, 7]\n",
    "        return F.log_softmax(x, dim=1) # 先softmax再取log\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "#模型训练\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)    #模型的输入有节点特征还有边特征,使用的是全部数据\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])   #损失仅仅计算的是训练集的损失\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "#模型测试\n",
    "model.eval()\n",
    "test_predict = model(data.x, data.edge_index)[data.test_mask] # 取测试集的预测结果\n",
    "max_index = torch.argmax(test_predict, dim=1)\n",
    "test_true = data.y[data.test_mask]\n",
    "correct = 0\n",
    "for i in range(len(max_index)):\n",
    "    if max_index[i] == test_true[i]:\n",
    "        correct += 1\n",
    "print('测试集准确率为：{}%'.format(correct*100/len(test_true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 边分类任务代码实现\n",
    "二分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6939, Acc: 0.5000\n",
      "Epoch: 002, Loss: 0.6877, Acc: 0.5149\n",
      "Epoch: 003, Loss: 0.6744, Acc: 0.6044\n",
      "Epoch: 004, Loss: 0.6700, Acc: 0.5322\n",
      "Epoch: 005, Loss: 0.6595, Acc: 0.5405\n",
      "Epoch: 006, Loss: 0.6547, Acc: 0.5987\n",
      "Epoch: 007, Loss: 0.6442, Acc: 0.6366\n",
      "Epoch: 008, Loss: 0.6392, Acc: 0.6089\n",
      "Epoch: 009, Loss: 0.6289, Acc: 0.6158\n",
      "Epoch: 010, Loss: 0.6267, Acc: 0.6697\n",
      "Epoch: 011, Loss: 0.6181, Acc: 0.6603\n",
      "Epoch: 012, Loss: 0.6092, Acc: 0.6411\n",
      "Epoch: 013, Loss: 0.6055, Acc: 0.6764\n",
      "Epoch: 014, Loss: 0.5984, Acc: 0.6714\n",
      "Epoch: 015, Loss: 0.5950, Acc: 0.6768\n",
      "Epoch: 016, Loss: 0.5886, Acc: 0.6842\n",
      "Epoch: 017, Loss: 0.5843, Acc: 0.6951\n",
      "Epoch: 018, Loss: 0.5847, Acc: 0.7083\n",
      "Epoch: 019, Loss: 0.5798, Acc: 0.6870\n",
      "Epoch: 020, Loss: 0.5724, Acc: 0.7114\n",
      "Epoch: 021, Loss: 0.5719, Acc: 0.7015\n",
      "Epoch: 022, Loss: 0.5702, Acc: 0.7098\n",
      "Epoch: 023, Loss: 0.5654, Acc: 0.6918\n",
      "Epoch: 024, Loss: 0.5653, Acc: 0.7093\n",
      "Epoch: 025, Loss: 0.5637, Acc: 0.7029\n",
      "Epoch: 026, Loss: 0.5600, Acc: 0.7008\n",
      "Epoch: 027, Loss: 0.5565, Acc: 0.7067\n",
      "Epoch: 028, Loss: 0.5624, Acc: 0.6965\n",
      "Epoch: 029, Loss: 0.5541, Acc: 0.7005\n",
      "Epoch: 030, Loss: 0.5619, Acc: 0.6941\n",
      "Epoch: 031, Loss: 0.5552, Acc: 0.7029\n",
      "Epoch: 032, Loss: 0.5531, Acc: 0.6986\n",
      "Epoch: 033, Loss: 0.5565, Acc: 0.7079\n",
      "Epoch: 034, Loss: 0.5536, Acc: 0.6974\n",
      "Epoch: 035, Loss: 0.5504, Acc: 0.7043\n",
      "Epoch: 036, Loss: 0.5539, Acc: 0.7027\n",
      "Epoch: 037, Loss: 0.5541, Acc: 0.7126\n",
      "Epoch: 038, Loss: 0.5495, Acc: 0.7116\n",
      "Epoch: 039, Loss: 0.5489, Acc: 0.6965\n",
      "Epoch: 040, Loss: 0.5549, Acc: 0.7083\n",
      "Epoch: 041, Loss: 0.5584, Acc: 0.7038\n",
      "Epoch: 042, Loss: 0.5473, Acc: 0.7022\n",
      "Epoch: 043, Loss: 0.5451, Acc: 0.7105\n",
      "Epoch: 044, Loss: 0.5536, Acc: 0.6951\n",
      "Epoch: 045, Loss: 0.5521, Acc: 0.7027\n",
      "Epoch: 046, Loss: 0.5525, Acc: 0.7053\n",
      "Epoch: 047, Loss: 0.5494, Acc: 0.7079\n",
      "Epoch: 048, Loss: 0.5465, Acc: 0.7010\n",
      "Epoch: 049, Loss: 0.5491, Acc: 0.7003\n",
      "Epoch: 050, Loss: 0.5489, Acc: 0.7000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "# 边分类模型\n",
    "class EdgeClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EdgeClassifier, self).__init__()\n",
    "        self.conv = GCNConv(in_channels, out_channels)\n",
    "        self.classifier = torch.nn.Linear(2 * out_channels, 2)  \n",
    "    def forward(self, x, edge_index):\n",
    "        # x: [2708, 1433], edge_index: [2, 10556]\n",
    "        x = F.relu(self.conv(x, edge_index)) # [2708, 64]\n",
    "        pos_edge_index = edge_index  # 正边[2, 10556]\n",
    "        total_edge_index = torch.cat([pos_edge_index, \n",
    "                        negative_sampling(pos_edge_index, num_neg_samples=pos_edge_index.size(1))],\n",
    "                        dim=1) # total_edge_index:[2, 21112]，21112前面一半是正边，后面一半是负边\n",
    "        edge_features = torch.cat([x[total_edge_index[0]], x[total_edge_index[1]]], \n",
    "                        dim=1)  # [21112, 2*out_channels]，21112前面一半是正边，后面一半是负边\n",
    "        return self.classifier(edge_features)\n",
    "\n",
    "# 加载数据集\n",
    "dataset = Planetoid(root='../data', name='Cora')\n",
    "data = dataset[0]\n",
    "# 重新设置train_mask和test_mask\n",
    "edges = data.edge_index.t().cpu().numpy()   # [10556, 2]\n",
    "num_edges = edges.shape[0] # 10556\n",
    "train_mask = torch.zeros(num_edges, dtype=torch.bool) # [10556]\n",
    "test_mask = torch.zeros(num_edges, dtype=torch.bool) #  [10556]\n",
    "train_size = int(0.8 * num_edges)\n",
    "train_indices = torch.randperm(num_edges)[:train_size] #生成0~10555的随机索引，取前80%作为训练集\n",
    "train_mask[train_indices] = True # 根据训练集索引设置train_mask\n",
    "test_mask[~train_mask] = True #\n",
    "\n",
    "# 定义模型和优化器/训练/测试\n",
    "model = EdgeClassifier(dataset.num_features, 64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(data.x, data.edge_index) # [21112, 2]\n",
    "    pos_edge_index = data.edge_index # 正边[2, 10556]\n",
    "    pos_labels = torch.ones(pos_edge_index.size(1), dtype=torch.long)  \n",
    "    neg_labels = torch.zeros(pos_edge_index.size(1), dtype=torch.long)  \n",
    "    labels = torch.cat([pos_labels, neg_labels], dim=0).to(logits.device) # 21112的前10556个是正边，后10556个是负边\n",
    "    new_train_mask = torch.cat([train_mask, train_mask], dim=0) # # 21112的前半和后半部分是一样的索引\n",
    "    loss = F.cross_entropy(logits[new_train_mask], labels[new_train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(data.x, data.edge_index)\n",
    "        pos_edge_index = data.edge_index\n",
    "        pos_labels = torch.ones(pos_edge_index.size(1), dtype=torch.long)\n",
    "        neg_labels = torch.zeros(pos_edge_index.size(1), dtype=torch.long)\n",
    "        labels = torch.cat([pos_labels, neg_labels], dim=0).to(logits.device)\n",
    "        new_test_mask = torch.cat([test_mask, test_mask], dim=0)\n",
    "        \n",
    "        predictions = logits[new_test_mask].max(1)[1] # 取概率最大的值的索引\n",
    "        correct = predictions.eq(labels[new_test_mask]).sum().item()\n",
    "        return correct / len(predictions)\n",
    "\n",
    "for epoch in range(0, 50):\n",
    "    loss = train()\n",
    "    acc = test()\n",
    "    print(f\"Epoch: {epoch+1:03d}, Loss: {loss:.4f}, Acc: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图分类任务代码实现\n",
    "蛋白酶结构的6分类任务，由600个图组成的，这些图实际上表示了不同的蛋白酶的结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.2130, Test Acc: 0.2667\n",
      "Epoch: 002, Train Acc: 0.2426, Test Acc: 0.1500\n",
      "Epoch: 003, Train Acc: 0.2407, Test Acc: 0.1833\n",
      "Epoch: 004, Train Acc: 0.2241, Test Acc: 0.1833\n",
      "Epoch: 005, Train Acc: 0.2315, Test Acc: 0.2167\n",
      "Epoch: 006, Train Acc: 0.2611, Test Acc: 0.1500\n",
      "Epoch: 007, Train Acc: 0.2630, Test Acc: 0.2500\n",
      "Epoch: 008, Train Acc: 0.2944, Test Acc: 0.2833\n",
      "Epoch: 009, Train Acc: 0.3019, Test Acc: 0.2500\n",
      "Epoch: 010, Train Acc: 0.2722, Test Acc: 0.2333\n",
      "Epoch: 011, Train Acc: 0.2889, Test Acc: 0.2167\n",
      "Epoch: 012, Train Acc: 0.2685, Test Acc: 0.2167\n",
      "Epoch: 013, Train Acc: 0.2630, Test Acc: 0.2000\n",
      "Epoch: 014, Train Acc: 0.2907, Test Acc: 0.2167\n",
      "Epoch: 015, Train Acc: 0.2981, Test Acc: 0.2000\n",
      "Epoch: 016, Train Acc: 0.2944, Test Acc: 0.1833\n",
      "Epoch: 017, Train Acc: 0.2870, Test Acc: 0.2333\n",
      "Epoch: 018, Train Acc: 0.2833, Test Acc: 0.1833\n",
      "Epoch: 019, Train Acc: 0.2852, Test Acc: 0.2500\n",
      "Epoch: 020, Train Acc: 0.2852, Test Acc: 0.2000\n",
      "Epoch: 021, Train Acc: 0.2889, Test Acc: 0.2000\n",
      "Epoch: 022, Train Acc: 0.2907, Test Acc: 0.2500\n",
      "Epoch: 023, Train Acc: 0.3093, Test Acc: 0.2000\n",
      "Epoch: 024, Train Acc: 0.2926, Test Acc: 0.2833\n",
      "Epoch: 025, Train Acc: 0.2981, Test Acc: 0.2167\n",
      "Epoch: 026, Train Acc: 0.3111, Test Acc: 0.3000\n",
      "Epoch: 027, Train Acc: 0.3241, Test Acc: 0.2333\n",
      "Epoch: 028, Train Acc: 0.2926, Test Acc: 0.2000\n",
      "Epoch: 029, Train Acc: 0.3241, Test Acc: 0.2833\n",
      "Epoch: 030, Train Acc: 0.3093, Test Acc: 0.2333\n",
      "Epoch: 031, Train Acc: 0.3241, Test Acc: 0.3000\n",
      "Epoch: 032, Train Acc: 0.2944, Test Acc: 0.2333\n",
      "Epoch: 033, Train Acc: 0.3000, Test Acc: 0.2667\n",
      "Epoch: 034, Train Acc: 0.3241, Test Acc: 0.2500\n",
      "Epoch: 035, Train Acc: 0.3093, Test Acc: 0.2167\n",
      "Epoch: 036, Train Acc: 0.3130, Test Acc: 0.2667\n",
      "Epoch: 037, Train Acc: 0.3185, Test Acc: 0.3000\n",
      "Epoch: 038, Train Acc: 0.3056, Test Acc: 0.2667\n",
      "Epoch: 039, Train Acc: 0.2944, Test Acc: 0.2833\n",
      "Epoch: 040, Train Acc: 0.2981, Test Acc: 0.2500\n",
      "Epoch: 041, Train Acc: 0.2944, Test Acc: 0.2167\n",
      "Epoch: 042, Train Acc: 0.2926, Test Acc: 0.2500\n",
      "Epoch: 043, Train Acc: 0.3148, Test Acc: 0.3000\n",
      "Epoch: 044, Train Acc: 0.3167, Test Acc: 0.2333\n",
      "Epoch: 045, Train Acc: 0.3241, Test Acc: 0.2833\n",
      "Epoch: 046, Train Acc: 0.3167, Test Acc: 0.2500\n",
      "Epoch: 047, Train Acc: 0.3037, Test Acc: 0.2333\n",
      "Epoch: 048, Train Acc: 0.3130, Test Acc: 0.2333\n",
      "Epoch: 049, Train Acc: 0.3204, Test Acc: 0.2500\n",
      "Epoch: 050, Train Acc: 0.3000, Test Acc: 0.2333\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# 加载数据集\n",
    "dataset = TUDataset(root='../data/ENZYMES', name='ENZYMES')\n",
    "dataset = dataset.shuffle()\n",
    "# x=[19580, 3], edge_index=[2, 74564], y=[600]\n",
    "train_dataset = dataset[:540] # x=[19580, 3], edge_index=[2, 74564], y=[600]\n",
    "test_dataset = dataset[540:] # x=[19580, 3], edge_index=[2, 74564], y=[600]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 定义图卷积网络模型\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = torch.nn.Linear(hidden_channels, dataset.num_classes)\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)    # batch指示了每个节点属于哪个图，再对每个图的节点特征求均值\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(train_loader):\n",
    "    model.train()\n",
    "    for data in train_loader: #  data: edge_index=[2, 8116], x=[2117, 3], y=[64], batch=[2117], ptr=[65]\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "for epoch in range(0,50):\n",
    "    train(train_loader)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch+1:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAT网络\n",
    "[Graph Attention Networks (GAT)](https://nn.labml.ai/graphs/gat/index.html)  \n",
    "[图注意网络GAT理解及Pytorch代码实现【PyGAT代码详细注释】](https://blog.csdn.net/weixin_43629813/article/details/129278266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from labml_helpers.module import Module\n",
    "\n",
    "\n",
    "class GraphAttentionLayer(Module):\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, n_heads: int,\n",
    "                is_concat: bool = True,\n",
    "                dropout: float = 0.6,\n",
    "                leaky_relu_negative_slope: float = 0.2):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_concat = is_concat\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Calculate the number of dimensions per head\n",
    "        if is_concat:\n",
    "            assert out_features % n_heads == 0\n",
    "            # If we are concatenating the multiple heads\n",
    "            self.n_hidden = out_features // n_heads\n",
    "        else:\n",
    "            # If we are averaging the multiple heads\n",
    "            self.n_hidden = out_features\n",
    "\n",
    "        # Linear layer for initial transformation;\n",
    "        # i.e. to transform the node embeddings before self-attention\n",
    "        self.linear = nn.Linear(in_features, self.n_hidden * n_heads, bias=False)\n",
    "        # Linear layer to compute attention score $e_{ij}$\n",
    "        self.attn = nn.Linear(self.n_hidden * 2, 1, bias=False)\n",
    "        # The activation for attention score $e_{ij}$\n",
    "        self.activation = nn.LeakyReLU(negative_slope=leaky_relu_negative_slope)\n",
    "        # Softmax to compute attention $\\alpha_{ij}$\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        # Dropout layer to be applied for attention\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, h: torch.Tensor, adj_mat: torch.Tensor):\n",
    "\n",
    "\n",
    "        n_nodes = h.shape[0]\n",
    "\n",
    "        g = self.linear(h).view(n_nodes, self.n_heads, self.n_hidden)\n",
    "\n",
    "        g_repeat = g.repeat(n_nodes, 1, 1)\n",
    "\n",
    "        g_repeat_interleave = g.repeat_interleave(n_nodes, dim=0)\n",
    "\n",
    "        g_concat = torch.cat([g_repeat_interleave, g_repeat], dim=-1)\n",
    "        # Reshape so that `g_concat[i, j]` is $\\overrightarrow{g_i} \\Vert \\overrightarrow{g_j}$\n",
    "        g_concat = g_concat.view(n_nodes, n_nodes, self.n_heads, 2 * self.n_hidden)\n",
    "\n",
    "        e = self.activation(self.attn(g_concat))\n",
    "        # Remove the last dimension of size `1`\n",
    "        e = e.squeeze(-1)\n",
    "\n",
    "        # The adjacency matrix should have shape\n",
    "        # `[n_nodes, n_nodes, n_heads]` or`[n_nodes, n_nodes, 1]`\n",
    "        assert adj_mat.shape[0] == 1 or adj_mat.shape[0] == n_nodes\n",
    "        assert adj_mat.shape[1] == 1 or adj_mat.shape[1] == n_nodes\n",
    "        assert adj_mat.shape[2] == 1 or adj_mat.shape[2] == self.n_heads\n",
    "        # Mask $e_{ij}$ based on adjacency matrix.\n",
    "        # $e_{ij}$ is set to $- \\infty$ if there is no edge from $i$ to $j$.\n",
    "        e = e.masked_fill(adj_mat == 0, float('-inf'))\n",
    "\n",
    "        a = self.softmax(e)\n",
    "\n",
    "        # Apply dropout regularization\n",
    "        a = self.dropout(a)\n",
    "\n",
    "        attn_res = torch.einsum('ijh,jhf->ihf', a, g)\n",
    "\n",
    "        # Concatenate the heads\n",
    "        if self.is_concat:\n",
    "            # $$\\overrightarrow{h'_i} = \\Bigg\\Vert_{k=1}^{K} \\overrightarrow{h'^k_i}$$\n",
    "            return attn_res.reshape(n_nodes, self.n_heads * self.n_hidden)\n",
    "        # Take the mean of the heads\n",
    "        else:\n",
    "            # $$\\overrightarrow{h'_i} = \\frac{1}{K} \\sum_{k=1}^{K} \\overrightarrow{h'^k_i}$$\n",
    "            return attn_res.mean(dim=1)\n",
    "        \n",
    "\n",
    "# Create an instance of the GAT model\n",
    "default_gat = GraphAttentionLayer(in_features=512, out_features=256, n_heads=1)\n",
    "time_gat = GraphAttentionLayer(in_features=512, out_features=256, n_heads=1)\n",
    "star_gat = GraphAttentionLayer(in_features=512, out_features=256, n_heads=1)\n",
    "\n",
    "\n",
    "# Create some sample input tensors\n",
    "# Node embeddings\n",
    "default_h = torch.randn(5, 512)  \n",
    "time_h = torch.randn(7, 512)  \n",
    "star_h = torch.randn(3, 512)  \n",
    "# Adjacency matrix\n",
    "default_adj = torch.ones(5, 5, 1)\n",
    "time_adj = torch.ones(7, 7, 1)\n",
    "star_adj = torch.ones(3, 3, 1)\n",
    "# Target node index\n",
    "default_index: int = default_adj.shape[0]//2 \n",
    "time_index: int = time_adj.shape[0]//2 \n",
    "star_index: int = star_adj.shape[0]//2\n",
    "\n",
    "# Forward pass through the GAT model\n",
    "node_default = default_gat(default_h, default_adj)[default_index]\n",
    "node_time = time_gat(time_h, time_adj)[time_index]  \n",
    "node_star = star_gat(star_h, star_adj)[star_index]\n",
    "node_all = 0.5*node_default + 0.25*node_time + 0.25*node_star\n",
    "\n",
    "# Print the output\n",
    "print(node_all.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
