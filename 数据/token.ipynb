{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- [NLP领域中的token和tokenization](https://www.zhihu.com/question/64984731/answer/3183726323)\n",
    "- [从词到数：Tokenizer与Embedding串讲](https://zhuanlan.zhihu.com/p/631463712)\n",
    "- Tokenization（分词） 在自然语言处理(NLP)的任务中是最基本的一步，把文本内容处理为最小基本单元即token用于后续的处理\n",
    "- 把文本处理成token有一系列的方法，基本思想是构建一个词表通过词表一一映射进行分词，以下以分词粒度为角度进行介绍如何构建合适的词表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word（词）粒度\n",
    "- 英语通过天然的分隔符进行分词，中文通过如jieba等进行分词\n",
    "- 优点：语义明确；上下文理解\n",
    "- 缺点：长尾效应和稀有词问题；OOV的问题；形态关系和词缀关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# char（字符）粒度\n",
    "- 字符为最小基本单元进行分词\n",
    "- 优点：统一处理方式；解决OOV问题\n",
    "- 缺点：语义信息不明确；处理效率低"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subword(子词)粒度\n",
    "- 既不希望将文本切分成单独的词（太大），也不想将其切分成单个字符（太小），而是希望得到介于词和字符之间的子词单元\n",
    "- 在BERT时代，`WordPiece` 分词方法被广泛应用，比如 BERT、DistilBERT等\n",
    "- 在大语言模型时代，最常用的分词方法是`Byte-Pair Encoding (BPE)`和`Byte-level BPE(BBPE)`\n",
    "- `Byte-Pair Encoding (BPE)`最初是一种文本压缩算法在15年被引入到NLP用于分词，GPT，RoBERTa等都采用了这种分词方法\n",
    "- `Byte-level BPE(BBPE)`是19年在BPE的基础上提出以Byte-level(字节)为粒度的分词方法，目前 GPT2，BLOOM，Llama，Falcon等采用该分词方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordPiece\n",
    "- 核心思想是将单词拆分成多个前缀符号（比如BERT中的##）最小单元，再通过子词合并规则将最小单元进行合并为子词级别\n",
    "- 核心步骤如下：\n",
    "  - 计算初始词表\n",
    "  - 计算合并分数：也称作互信息（信息论中衡量两个变量之间的关联程度），公式为`分数 = 合并pair候选的频率 / (第一个元素的频率 × 第二个元素的频率)`\n",
    "  - 合并分数最高的子词对\n",
    "  - 重复合并步骤\n",
    "  - 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"我\",\n",
    "    \"喜欢\",\n",
    "    \"吃\",\n",
    "    \"苹果\",\n",
    "    \"他\",\n",
    "    \"不\",\n",
    "    \"喜欢\",\n",
    "    \"吃\",\n",
    "    \"苹果派\",\n",
    "    \"I like to eat apples\",\n",
    "    \"She has a cute cat\",\n",
    "    \"you are very cute\",\n",
    "    \"give you a hug\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats: defaultdict(<class 'int'>, {'我': 1, '喜欢': 2, '吃': 2, '苹果': 1, '他': 1, '不': 1, '苹果派': 1, 'I': 1, 'like': 1, 'to': 1, 'eat': 1, 'apples': 1, 'She': 1, 'has': 1, 'a': 2, 'cute': 2, 'cat': 1, 'you': 2, 'are': 1, 'very': 1, 'give': 1, 'hug': 1})\n",
      "alphabet: ['##a', '##e', '##g', '##h', '##i', '##k', '##l', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##y', '##果', '##欢', '##派', 'I', 'S', 'a', 'c', 'e', 'g', 'h', 'l', 't', 'v', 'y', '不', '他', '吃', '喜', '我', '苹']\n"
     ]
    }
   ],
   "source": [
    "# 统计每个词出现的频率并初始化初始词表\n",
    "from collections import defaultdict\n",
    "\n",
    "def bulid_stats(sentences):\n",
    "    stats = defaultdict(int) #所有键的值默认为0\n",
    "    for sentence in sentences:\n",
    "        symbols = sentence.split() #使用空格作为分隔符  \n",
    "        for symbol in symbols:\n",
    "            stats[symbol]+=1\n",
    "    return stats\n",
    "\n",
    "stats = bulid_stats(sentences)\n",
    "print(\"stats:\",stats)\n",
    "\n",
    "alphabet = []\n",
    "for word in stats.keys():\n",
    "    if word[0] not in alphabet:\n",
    "        alphabet.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        if f\"##{letter}\" not in alphabet: # \n",
    "            alphabet.append(f\"##{letter}\")\n",
    "alphabet.sort()\n",
    "\n",
    "vocab = alphabet.copy()\n",
    "print(\"alphabet:\",alphabet)\n",
    "\n",
    "# 根据初始词表拆分每个词\n",
    "splits = {\n",
    "    word:[c if i==0 else f\"##{c}\" for i,c in  enumerate(word)]\n",
    "    for word in stats.keys()\n",
    "}    \n",
    "print(\"splits:\", splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('喜', '##欢'): 0.5\n",
      "('苹', '##果'): 0.5\n",
      "('##果', '##派'): 0.5\n",
      "('l', '##i'): 0.5\n",
      "('##i', '##k'): 0.5\n",
      "('##k', '##e'): 0.125\n"
     ]
    }
   ],
   "source": [
    "# 根据上述提到的计算互信息的分数公式进行计算\n",
    "def compute_pair_scores(splits):\n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word,freq in stats.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]]+=freq\n",
    "            continue\n",
    "        for i in range(len(split)-1):\n",
    "            pair = (split[i],split[i+1])\n",
    "            letter_freqs[split[i]]+=freq\n",
    "            pair_freqs[pair]+=freq\n",
    "        letter_freqs[split[-1]]+=freq\n",
    "\n",
    "    score = {\n",
    "        pair: freq/(letter_freqs[pair[0]]*letter_freqs[pair[1]])\n",
    "        for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "pair_scores = compute_pair_scores(splits)\n",
    "for i, key in enumerate(pair_scores.keys()):\n",
    "    print(f\"{key}: {pair_scores[key]}\")\n",
    "    if i >= 5:\n",
    "        break\n",
    "    \n",
    "# 将分数最高的进行合并然后开始循环迭代，看一看分数最高的pair（子词对)\n",
    "best_pair = \"\"\n",
    "max_score = 0\n",
    "for pair, score in pair_scores.items():\n",
    "    if score > max_score:\n",
    "        best_pair = pair\n",
    "        max_score = score\n",
    "print(best_pair,max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: ['##a', '##e', '##g', '##h', '##i', '##k', '##l', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##y', '##果', '##欢', '##派', 'I', 'S', 'a', 'c', 'e', 'g', 'h', 'l', 't', 'v', 'y', '不', '他', '吃', '喜', '我', '苹', 'Sh', '喜欢', '苹果', '苹果派', 'li', 'lik', 'gi', 'giv', '##pl', '##ppl', '##ry', 'to', 'yo', 'ea', 'eat']\n"
     ]
    }
   ],
   "source": [
    "# 最后就是一直进行循环迭代，直到vocab达到了我们想要的数量\n",
    "def merge_pair(pair1,pair2,splits):\n",
    "    for word in stats:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == pair1 and split[i+1] == pair2:\n",
    "                merge = pair1 + pair2[2:] if pair2.startswith(\"##\") else pair1+pair2\n",
    "                split = split[:i] + [merge] + split[i+2:]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits  \n",
    "\n",
    "vocab_size = 50\n",
    "while len(vocab) < vocab_size:\n",
    "    score = compute_pair_scores(splits)\n",
    "    best_pair,max_score = \"\",0\n",
    "    for pair,score in score.items():\n",
    "        if  score > max_score:\n",
    "            best_pair,max_score = pair,score\n",
    "    splits = merge_pair(*best_pair,splits)\n",
    "    new_token = (\n",
    "        best_pair[0]+best_pair[1][2:]\n",
    "        if best_pair[1].startswith('##') \n",
    "        else best_pair[0]+best_pair[1]\n",
    "    )\n",
    "    vocab.append(new_token)\n",
    "\n",
    "print(\"vocab:\", vocab)\n",
    "\n",
    "all_vocab = vocab + [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]# + other_alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding (BPE)\n",
    "- 核心思想是逐步合并出现频率最高的子词对而不是像Wordpiece计算合并分数，从而构建出一个词汇表\n",
    "- 核心步骤如下：\n",
    "  - 计算初始词表\n",
    "  - 构建频率统计\n",
    "  - 合并频率最高的子词对\n",
    "  - 重复合并步骤\n",
    "  - 分词\n",
    "- 当词汇表的大小受限时，一些较少频繁出现的子词和没有在训练过程中见过的子词，就会无法进入词汇表出现`OOV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"我\",\n",
    "    \"喜欢\",\n",
    "    \"吃\",\n",
    "    \"苹果\",\n",
    "    \"他\",\n",
    "    \"不\",\n",
    "    \"喜欢\",\n",
    "    \"吃\",\n",
    "    \"苹果派\",\n",
    "    \"I like to eat apples\",\n",
    "    \"She has a cute cat\",\n",
    "    \"you are very cute\",\n",
    "    \"give you a hug\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats: defaultdict(<class 'int'>, {'我': 1, '喜欢': 2, '吃': 2, '苹果': 1, '他': 1, '不': 1, '苹果派': 1, 'I': 1, 'like': 1, 'to': 1, 'eat': 1, 'apples': 1, 'She': 1, 'has': 1, 'a': 2, 'cute': 2, 'cat': 1, 'you': 2, 'are': 1, 'very': 1, 'give': 1, 'hug': 1})\n",
      "alphabet: ['I', 'S', 'a', 'c', 'e', 'g', 'h', 'i', 'k', 'l', 'o', 'p', 'r', 's', 't', 'u', 'v', 'y', '不', '他', '吃', '喜', '我', '果', '欢', '派', '苹']\n",
      "splits: {'我': ['我'], '喜欢': ['喜', '欢'], '吃': ['吃'], '苹果': ['苹', '果'], '他': ['他'], '不': ['不'], '苹果派': ['苹', '果', '派'], 'I': ['I'], 'like': ['l', 'i', 'k', 'e'], 'to': ['t', 'o'], 'eat': ['e', 'a', 't'], 'apples': ['a', 'p', 'p', 'l', 'e', 's'], 'She': ['S', 'h', 'e'], 'has': ['h', 'a', 's'], 'a': ['a'], 'cute': ['c', 'u', 't', 'e'], 'cat': ['c', 'a', 't'], 'you': ['y', 'o', 'u'], 'are': ['a', 'r', 'e'], 'very': ['v', 'e', 'r', 'y'], 'give': ['g', 'i', 'v', 'e'], 'hug': ['h', 'u', 'g']}\n"
     ]
    }
   ],
   "source": [
    "# 统计每个词出现的频率并初始化初始词表\n",
    "\n",
    "# 构建频率统计\n",
    "from  collections import defaultdict\n",
    "def build_stats(sentences):\n",
    "    stats = defaultdict(int)\n",
    "    for sentence in sentences:\n",
    "        symbols = sentence.split()\n",
    "        for symbol in symbols:\n",
    "            stats[symbol] += 1\n",
    "    return stats\n",
    "stats = build_stats(sentences)\n",
    "print(\"stats:\", stats)\n",
    "\n",
    "alphabet = []\n",
    "for word in stats.keys():\n",
    "    for letter in word:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "alphabet.sort()\n",
    "\n",
    "# 初始词表\n",
    "vocab = alphabet.copy()\n",
    "print(\"alphabet:\", alphabet)\n",
    "\n",
    "# 根据初始词表拆分每个词，计算左右pair(子词对)出现的频率\n",
    "splits = {word: [c for c in word]  for word in stats.keys()}\n",
    "print('splits:',splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('喜', '欢'): 2\n",
      "('苹', '果'): 2\n",
      "('果', '派'): 1\n",
      "('l', 'i'): 1\n",
      "('i', 'k'): 1\n",
      "('k', 'e'): 1\n",
      "('喜', '欢') 2\n"
     ]
    }
   ],
   "source": [
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word,freq in stats.items():\n",
    "        split = splits[word]\n",
    "        if len(split)==1:\n",
    "            continue\n",
    "        for i in range(len(split)-1):\n",
    "            pair_freqs[(split[i], split[i+1])] += freq\n",
    "    return pair_freqs\n",
    "\n",
    "\n",
    "pair_freqs = compute_pair_freqs(splits)\n",
    "for i,key in enumerate(pair_freqs.keys()):\n",
    "    print(f\"{key}: {pair_freqs[key]}\")\n",
    "    if i >= 5:\n",
    "        break\n",
    "\n",
    "# 然后开始循环迭代找到出现频率最高的pair(子词对)\n",
    "best_pair,max_freq=\"\",0\n",
    "for pair,freq in pair_freqs.items():\n",
    "    if freq > max_freq:\n",
    "        best_pair,max_freq = pair,freq\n",
    "print(best_pair,max_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merges: {('喜', '欢'): '喜欢', ('苹', '果'): '苹果', ('a', 't'): 'at', ('c', 'u'): 'cu', ('cu', 't'): 'cut', ('cut', 'e'): 'cute', ('y', 'o'): 'yo', ('yo', 'u'): 'you', ('v', 'e'): 've', ('苹果', '派'): '苹果派', ('l', 'i'): 'li', ('li', 'k'): 'lik', ('lik', 'e'): 'like', ('t', 'o'): 'to', ('e', 'at'): 'eat', ('a', 'p'): 'ap', ('ap', 'p'): 'app', ('app', 'l'): 'appl', ('appl', 'e'): 'apple', ('apple', 's'): 'apples', ('S', 'h'): 'Sh', ('Sh', 'e'): 'She', ('h', 'a'): 'ha'}\n",
      "vocab: ['I', 'S', 'a', 'c', 'e', 'g', 'h', 'i', 'k', 'l', 'o', 'p', 'r', 's', 't', 'u', 'v', 'y', '不', '他', '吃', '喜', '我', '果', '欢', '派', '苹', '喜欢', '苹果', 'at', 'cu', 'cut', 'cute', 'yo', 'you', 've', '苹果派', 'li', 'lik', 'like', 'to', 'eat', 'ap', 'app', 'appl', 'apple', 'apples', 'Sh', 'She', 'ha']\n",
      "splits: {'我': ['我'], '喜欢': ['喜欢'], '吃': ['吃'], '苹果': ['苹果'], '他': ['他'], '不': ['不'], '苹果派': ['苹果派'], 'I': ['I'], 'like': ['like'], 'to': ['to'], 'eat': ['eat'], 'apples': ['apples'], 'She': ['She'], 'has': ['ha', 's'], 'a': ['a'], 'cute': ['cute'], 'cat': ['c', 'at'], 'you': ['you'], 'are': ['a', 'r', 'e'], 'very': ['ve', 'r', 'y'], 'give': ['g', 'i', 've'], 'hug': ['h', 'u', 'g']}\n"
     ]
    }
   ],
   "source": [
    "def merge_pair(a,b,splits):\n",
    "    for word in stats.keys():\n",
    "        split = splits[word]\n",
    "        if len(split)==1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split)-1:\n",
    "            if split[i] == a and split[i+1] == b:\n",
    "                split = split[:i]+[a+b]+split[i+2:]\n",
    "            else: i+=1\n",
    "        splits[word] = split\n",
    "    return splits\n",
    "\n",
    "# 一直进行循环直到vocab达到了我们想要的数量\n",
    "merges = {}\n",
    "vocab_size = 50\n",
    "\n",
    "while len(vocab)<vocab_size:\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    best_pair,max_freq=\"\",0\n",
    "    for pair,freq in pair_freqs.items():\n",
    "        if freq > max_freq:\n",
    "            best_pair,max_freq = pair,freq\n",
    "    splits = merge_pair(*best_pair,splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])\n",
    "\n",
    "print(\"merges:\", merges)\n",
    "print(\"vocab:\", vocab)\n",
    "print(\"splits:\", splits)\n",
    "\n",
    "all_vocab = vocab + [\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"] #+ other_alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-level BPE(BBPE)\n",
    "- Unicode 是字符集，为每个字符分配唯一的代码点\n",
    "- UTF-8 是一种基于 Unicode 的字符编码方式，用于在计算机中存储和传输字符\n",
    "- Byte(字节)：计算机存储和数据处理时，字节是最小的单位，一个字节包含8个(Bit)二进制位,一个字节能表示的范围是`256`\n",
    "- `BPE`是最小词汇是字符级别，而`BBPE`是字节级别的，通过`UTF-8`的编码方式，理论上可以表示世界上所有字符\n",
    "- 核心步骤和`BPE`除了实现粒度不一样，其他都一样:\n",
    "  - 计算初始词表\n",
    "  - 构建频率统计\n",
    "  - 合并频率最高的子词对\n",
    "  - 重复合并步骤\n",
    "  - 分词\n",
    "- 现在的大模型中，以`Byte-level BPE(BBPE)`这种方式进行分词是不会出现`OOV`\n",
    "- 但词表中如果没有word级别的词的话，一些中英文就会分词分的很细碎(比如Llama),于是出现了扩充Llama中文词表的工作\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_vocab: [b'\\x00', b'\\x01', b'\\x02', b'\\x03', b'\\x04', b'\\x05', b'\\x06', b'\\x07', b'\\x08', b'\\t', b'\\n', b'\\x0b', b'\\x0c', b'\\r', b'\\x0e', b'\\x0f', b'\\x10', b'\\x11', b'\\x12', b'\\x13', b'\\x14', b'\\x15', b'\\x16', b'\\x17', b'\\x18', b'\\x19', b'\\x1a', b'\\x1b', b'\\x1c', b'\\x1d', b'\\x1e', b'\\x1f', b' ', b'!', b'\"', b'#', b'$', b'%', b'&', b\"'\", b'(', b')', b'*', b'+', b',', b'-', b'.', b'/', b'0', b'1', b'2', b'3', b'4', b'5', b'6', b'7', b'8', b'9', b':', b';', b'<', b'=', b'>', b'?', b'@', b'A', b'B', b'C', b'D', b'E', b'F', b'G', b'H', b'I', b'J', b'K', b'L', b'M', b'N', b'O', b'P', b'Q', b'R', b'S', b'T', b'U', b'V', b'W', b'X', b'Y', b'Z', b'[', b'\\\\', b']', b'^', b'_', b'`', b'a', b'b', b'c', b'd', b'e', b'f', b'g', b'h', b'i', b'j', b'k', b'l', b'm', b'n', b'o', b'p', b'q', b'r', b's', b't', b'u', b'v', b'w', b'x', b'y', b'z', b'{', b'|', b'}', b'~', b'\\x7f', b'\\x80', b'\\x81', b'\\x82', b'\\x83', b'\\x84', b'\\x85', b'\\x86', b'\\x87', b'\\x88', b'\\x89', b'\\x8a', b'\\x8b', b'\\x8c', b'\\x8d', b'\\x8e', b'\\x8f', b'\\x90', b'\\x91', b'\\x92', b'\\x93', b'\\x94', b'\\x95', b'\\x96', b'\\x97', b'\\x98', b'\\x99', b'\\x9a', b'\\x9b', b'\\x9c', b'\\x9d', b'\\x9e', b'\\x9f', b'\\xa0', b'\\xa1', b'\\xa2', b'\\xa3', b'\\xa4', b'\\xa5', b'\\xa6', b'\\xa7', b'\\xa8', b'\\xa9', b'\\xaa', b'\\xab', b'\\xac', b'\\xad', b'\\xae', b'\\xaf', b'\\xb0', b'\\xb1', b'\\xb2', b'\\xb3', b'\\xb4', b'\\xb5', b'\\xb6', b'\\xb7', b'\\xb8', b'\\xb9', b'\\xba', b'\\xbb', b'\\xbc', b'\\xbd', b'\\xbe', b'\\xbf', b'\\xc0', b'\\xc1', b'\\xc2', b'\\xc3', b'\\xc4', b'\\xc5', b'\\xc6', b'\\xc7', b'\\xc8', b'\\xc9', b'\\xca', b'\\xcb', b'\\xcc', b'\\xcd', b'\\xce', b'\\xcf', b'\\xd0', b'\\xd1', b'\\xd2', b'\\xd3', b'\\xd4', b'\\xd5', b'\\xd6', b'\\xd7', b'\\xd8', b'\\xd9', b'\\xda', b'\\xdb', b'\\xdc', b'\\xdd', b'\\xde', b'\\xdf', b'\\xe0', b'\\xe1', b'\\xe2', b'\\xe3', b'\\xe4', b'\\xe5', b'\\xe6', b'\\xe7', b'\\xe8', b'\\xe9', b'\\xea', b'\\xeb', b'\\xec', b'\\xed', b'\\xee', b'\\xef', b'\\xf0', b'\\xf1', b'\\xf2', b'\\xf3', b'\\xf4', b'\\xf5', b'\\xf6', b'\\xf7', b'\\xf8', b'\\xf9', b'\\xfa', b'\\xfb', b'\\xfc', b'\\xfd', b'\\xfe', b'\\xff']\n",
      "stats: defaultdict(<class 'int'>, {b'\\xe6\\x88\\x91': 1, b'\\xe5\\x96\\x9c\\xe6\\xac\\xa2': 2, b'\\xe5\\x90\\x83': 2, b'\\xe8\\x8b\\xb9\\xe6\\x9e\\x9c': 1, b'\\xe4\\xbb\\x96': 1, b'\\xe4\\xb8\\x8d': 1, b'\\xe8\\x8b\\xb9\\xe6\\x9e\\x9c\\xe6\\xb4\\xbe': 1, b'I': 1, b'like': 1, b'to': 1, b'eat': 1, b'apples': 1, b'She': 1, b'has': 1, b'a': 2, b'cute': 2, b'cat': 1, b'you': 2, b'are': 1, b'very': 1, b'give': 1, b'hug': 1})\n",
      "splits: {b'\\xe6\\x88\\x91': [230, 136, 145], b'\\xe5\\x96\\x9c\\xe6\\xac\\xa2': [229, 150, 156, 230, 172, 162], b'\\xe5\\x90\\x83': [229, 144, 131], b'\\xe8\\x8b\\xb9\\xe6\\x9e\\x9c': [232, 139, 185, 230, 158, 156], b'\\xe4\\xbb\\x96': [228, 187, 150], b'\\xe4\\xb8\\x8d': [228, 184, 141], b'\\xe8\\x8b\\xb9\\xe6\\x9e\\x9c\\xe6\\xb4\\xbe': [232, 139, 185, 230, 158, 156, 230, 180, 190], b'I': [73], b'like': [108, 105, 107, 101], b'to': [116, 111], b'eat': [101, 97, 116], b'apples': [97, 112, 112, 108, 101, 115], b'She': [83, 104, 101], b'has': [104, 97, 115], b'a': [97], b'cute': [99, 117, 116, 101], b'cat': [99, 97, 116], b'you': [121, 111, 117], b'are': [97, 114, 101], b'very': [118, 101, 114, 121], b'give': [103, 105, 118, 101], b'hug': [104, 117, 103]}\n",
      "vocab: [b'\\x00', b'\\x01', b'\\x02', b'\\x03', b'\\x04', b'\\x05', b'\\x06', b'\\x07', b'\\x08', b'\\t', b'\\n', b'\\x0b', b'\\x0c', b'\\r', b'\\x0e', b'\\x0f', b'\\x10', b'\\x11', b'\\x12', b'\\x13', b'\\x14', b'\\x15', b'\\x16', b'\\x17', b'\\x18', b'\\x19', b'\\x1a', b'\\x1b', b'\\x1c', b'\\x1d', b'\\x1e', b'\\x1f', b' ', b'!', b'\"', b'#', b'$', b'%', b'&', b\"'\", b'(', b')', b'*', b'+', b',', b'-', b'.', b'/', b'0', b'1', b'2', b'3', b'4', b'5', b'6', b'7', b'8', b'9', b':', b';', b'<', b'=', b'>', b'?', b'@', b'A', b'B', b'C', b'D', b'E', b'F', b'G', b'H', b'I', b'J', b'K', b'L', b'M', b'N', b'O', b'P', b'Q', b'R', b'S', b'T', b'U', b'V', b'W', b'X', b'Y', b'Z', b'[', b'\\\\', b']', b'^', b'_', b'`', b'a', b'b', b'c', b'd', b'e', b'f', b'g', b'h', b'i', b'j', b'k', b'l', b'm', b'n', b'o', b'p', b'q', b'r', b's', b't', b'u', b'v', b'w', b'x', b'y', b'z', b'{', b'|', b'}', b'~', b'\\x7f', b'\\x80', b'\\x81', b'\\x82', b'\\x83', b'\\x84', b'\\x85', b'\\x86', b'\\x87', b'\\x88', b'\\x89', b'\\x8a', b'\\x8b', b'\\x8c', b'\\x8d', b'\\x8e', b'\\x8f', b'\\x90', b'\\x91', b'\\x92', b'\\x93', b'\\x94', b'\\x95', b'\\x96', b'\\x97', b'\\x98', b'\\x99', b'\\x9a', b'\\x9b', b'\\x9c', b'\\x9d', b'\\x9e', b'\\x9f', b'\\xa0', b'\\xa1', b'\\xa2', b'\\xa3', b'\\xa4', b'\\xa5', b'\\xa6', b'\\xa7', b'\\xa8', b'\\xa9', b'\\xaa', b'\\xab', b'\\xac', b'\\xad', b'\\xae', b'\\xaf', b'\\xb0', b'\\xb1', b'\\xb2', b'\\xb3', b'\\xb4', b'\\xb5', b'\\xb6', b'\\xb7', b'\\xb8', b'\\xb9', b'\\xba', b'\\xbb', b'\\xbc', b'\\xbd', b'\\xbe', b'\\xbf', b'\\xc0', b'\\xc1', b'\\xc2', b'\\xc3', b'\\xc4', b'\\xc5', b'\\xc6', b'\\xc7', b'\\xc8', b'\\xc9', b'\\xca', b'\\xcb', b'\\xcc', b'\\xcd', b'\\xce', b'\\xcf', b'\\xd0', b'\\xd1', b'\\xd2', b'\\xd3', b'\\xd4', b'\\xd5', b'\\xd6', b'\\xd7', b'\\xd8', b'\\xd9', b'\\xda', b'\\xdb', b'\\xdc', b'\\xdd', b'\\xde', b'\\xdf', b'\\xe0', b'\\xe1', b'\\xe2', b'\\xe3', b'\\xe4', b'\\xe5', b'\\xe6', b'\\xe7', b'\\xe8', b'\\xe9', b'\\xea', b'\\xeb', b'\\xec', b'\\xed', b'\\xee', b'\\xef', b'\\xf0', b'\\xf1', b'\\xf2', b'\\xf3', b'\\xf4', b'\\xf5', b'\\xf6', b'\\xf7', b'\\xf8', b'\\xf9', b'\\xfa', b'\\xfb', b'\\xfc', b'\\xfd', b'\\xfe', b'\\xff']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "sentences = [\n",
    "    \"我\",\n",
    "    \"喜欢\",\n",
    "    \"吃\",\n",
    "    \"苹果\",\n",
    "    \"他\",\n",
    "    \"不\",\n",
    "    \"喜欢\",\n",
    "    \"吃\",\n",
    "    \"苹果派\",\n",
    "    \"I like to eat apples\",\n",
    "    \"She has a cute cat\",\n",
    "    \"you are very cute\",\n",
    "    \"give you a hug\",\n",
    "]\n",
    "# 构建初始词汇表，包含一个字节的256个表示\n",
    "initial_vocab = [bytes([byte]) for byte in range(256)]\n",
    "vocab = initial_vocab.copy()\n",
    "print(\"initial_vocab:\", initial_vocab)\n",
    "\n",
    "# 构建频率统计\n",
    "def build_stats(sentences):\n",
    "    stats = defaultdict(int)\n",
    "    for sentence in sentences:\n",
    "        symbols = sentence.split()\n",
    "        for symbol in symbols:\n",
    "            stats[symbol.encode(\"utf-8\")] += 1\n",
    "    return stats\n",
    "stats = build_stats(sentences)\n",
    "print(\"stats:\",stats)\n",
    "\n",
    "splits = {word: [byte for byte in word] for word in stats.keys()}\n",
    "print(\"splits:\",splits)\n",
    "\n",
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in stats.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs\n",
    "\n",
    "# pair_freqs = compute_pair_freqs(splits)\n",
    "\n",
    "def merge_pair(pair, splits):\n",
    "    merged_byte = bytes(pair)\n",
    "    for word in stats:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i:i+2] == pair:  # 检查分割中是否有这对字节\n",
    "                split = split[:i] + [merged_byte] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits\n",
    "\n",
    "vocab_size = 50\n",
    "while len(vocab) < vocab_size:\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    best_pair = ()\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if max_freq is None or max_freq < freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    splits = merge_pair(best_pair, splits)\n",
    "    merged_byte = bytes(best_pair)\n",
    "\n",
    "print(\"vocab:\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "## 中文的词向量\n",
    "Chinese Word Vectors 中文词向量:https://github.com/Embedding/Chinese-Word-Vectors\n",
    "## MTEB排行\n",
    "MTEB: https://huggingface.co/spaces/mteb/leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('aspire/acge_text_embedding')\n",
    "texts1 = [\"胡子长得太快怎么办？\"]\n",
    "texts2 = [\"胡子长得快怎么办？\", \"怎样使胡子不浓密！\", \"香港买手表哪里好\", \"在杭州手机到哪里买\"]\n",
    "embs1 = model.encode(texts1, normalize_embeddings=True)\n",
    "embs2 = model.encode(texts2, normalize_embeddings=True)\n",
    "similarity = embs1 @ embs2.T\n",
    "print(similarity) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取 texts1[i] 对应的最相似 texts2[j]\n",
    "for i in range(len(texts1)):\n",
    "    scores = []\n",
    "    for j in range(len(texts2)):\n",
    "        scores.append([texts2[j], similarity[i][j]])\n",
    "    scores = sorted(scores, key=lambda x:x[1], reverse=True)\n",
    "\n",
    "    print(f\"查询文本：{texts1[i]}\")\n",
    "    for text2, score in scores:\n",
    "        print(f\"相似文本：{text2}，打分：{score}\")\n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## milvus工具\n",
    "- 向量数据库：专门为存储和检索向量数据而设计的数据库\n",
    "- Milvus 是一款云原生向量数据库，它具备高可用、高性能、易拓展的特点，用于海量向量数据的实时召回\n",
    "- [milvus官网](https://milvus.io/)\n",
    "- [为AI而生的数据库：Milvus详解及实战](https://blog.csdn.net/lsb2002/article/details/132222947)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic\n",
    "- [NLP关键词提取必备：从TFIDF到KeyBert范式原理优缺点与开源实现](https://mp.weixin.qq.com/s/H2aRyF3tNZCCOxa4Oy2JMw)\n",
    "- [盘点 KeyBert、TextRank 等九种主流关键词提取算法原理及 Python 代码实现](https://zhuanlan.zhihu.com/p/568271135)\n",
    "- [NLP关键词提取方法总结及实现](https://blog.csdn.net/asialee_bird/article/details/96454544)\n",
    "  \n",
    "## 基于词袋加权的TFIDF算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.analyse\n",
    "text = \"美国总统拜登当地时间26日又语出惊人。据路透社报道，拜登当天在波兰的演 讲中，称俄罗斯总统普京“不能继续掌权了”。\\\n",
    "    白宫官员随后出来解释，克里姆林宫同日 也作出回应，称“这不是由拜登决定的”。报道称，在华沙皇家城堡发表的讲话中，拜登在\\\n",
    "    谴责普京后表示“看在上帝的份上，这个人不能继续掌权了”。对此，路透社表示，该言论引发了华盛顿方面对局势升级的担忧，美国\\\n",
    "    一直避免直接对乌克兰进行军事干预，并明确表示不支持政权更迭。随后，一名白宫官员对拜登这番话进行解释，称拜登的言论并不\\\n",
    "    代表华盛顿政策的转变，其目的是让世界为乌克兰问题上的长期冲突做好准备。该官员称，拜登意为“不能 允许普京对其邻国或该地区\\\n",
    "    行使权力”，而不是在讨论普京在俄罗斯的权力或俄 罗斯的政权 更迭问题。同日，路透社称，被问及拜登这一言论时，俄罗斯总统新闻\\\n",
    "    秘书佩斯科夫作出回应，称“这不是由拜登决定的。俄罗斯总统是由俄罗斯人选举产生的”。此前 ，俄罗斯安全会议副主席梅德韦杰夫接受\\\n",
    "    俄媒采访时曾表示，俄特别军事行动既定目标包括实现乌克兰去军事化、去纳粹化、成为中立国家、不奉行反俄政策。俄开展特别军事行动\\\n",
    "    首先是因为其设定的目标未能通过外交方式实现。他表示，社会调查显示，四分之三的俄罗斯民众支持在乌开展特别军事行动。西方国家企\\\n",
    "    图通过制裁影响俄民众，煽动他们反对政府，结果适得其反。\"\n",
    "# TF-IDF\n",
    "jieba.analyse.extract_tags(text, topK=20, withWeight=True, allowPOS=('ns', 'n', 'vn', 'v','nr', 'nt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入语料库\n",
    "corpus = ['this is the first document',\n",
    "        'this is the second second document',\n",
    "        'and the third one',\n",
    "        'is this the first document']\n",
    "\n",
    "words_list = list()\n",
    "for i in range(len(corpus)):\n",
    "    words_list.append(corpus[i].split(' '))\n",
    "### 第一种 gensim\n",
    "from gensim import corpora\n",
    "# 赋给语料库中每个词(不重复的词)一个整数id\n",
    "dic = corpora.Dictionary(words_list)\n",
    "# print(dic.token2id)\n",
    "# 元组中第一个元素是词语在词典中对应的id，第二个元素是词语在文档中出现的次数\n",
    "new_corpus = [dic.doc2bow(words) for words in words_list]\n",
    "\n",
    "# 训练模型并保存\n",
    "from gensim import models\n",
    "tfidf = models.TfidfModel(new_corpus)\n",
    "tfidf_vec = []\n",
    "for i in range(len(corpus)):\n",
    "    string = corpus[i]\n",
    "    string_bow = dic.doc2bow(string.lower().split())\n",
    "    string_tfidf = tfidf[string_bow]\n",
    "    tfidf_vec.append(string_tfidf)\n",
    "# 输出 词语id与词语tfidf值\n",
    "print(tfidf_vec)\n",
    "\n",
    "# ### 第二种 sklearn\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# tfidf_vec = TfidfVectorizer()\n",
    "# tfidf_matrix = tfidf_vec.fit_transform(corpus)\n",
    "# # 得到语料库所有不重复的词\n",
    "# print(tfidf_vec.get_feature_names())\n",
    "# # 得到每个单词对应的id值\n",
    "# print(tfidf_vec.vocabulary_)\n",
    "# # 得到每个句子所对应的向量，向量里数字的顺序是按照词语的id顺序来的\n",
    "# print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 考虑词关联网络的TextRank算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.analyse\n",
    "text = \"美国总统拜登当地时间26日又语出惊人。据路透社报道，拜登当天在波兰的演 讲中，称俄罗斯总统普京“不能继续掌权了”。\\\n",
    "    白宫官员随后出来解释，克里姆林宫同日 也作出回应，称“这不是由拜登决定的”。报道称，在华沙皇家城堡发表的讲话中，拜登在\\\n",
    "    谴责普京后表示“看在上帝的份上，这个人不能继续掌权了”。对此，路透社表示，该言论引发了华盛顿方面对局势升级的担忧，美国\\\n",
    "    一直避免直接对乌克兰进行军事干预，并明确表示不支持政权更迭。随后，一名白宫官员对拜登这番话进行解释，称拜登的言论并不\\\n",
    "    代表华盛顿政策的转变，其目的是让世界为乌克兰问题上的长期冲突做好准备。该官员称，拜登意为“不能 允许普京对其邻国或该地区\\\n",
    "    行使权力”，而不是在讨论普京在俄罗斯的权力或俄 罗斯的政权 更迭问题。同日，路透社称，被问及拜登这一言论时，俄罗斯总统新闻\\\n",
    "    秘书佩斯科夫作出回应，称“这不是由拜登决定的。俄罗斯总统是由俄罗斯人选举产生的”。此前 ，俄罗斯安全会议副主席梅德韦杰夫接受\\\n",
    "    俄媒采访时曾表示，俄特别军事行动既定目标包括实现乌克兰去军事化、去纳粹化、成为中立国家、不奉行反俄政策。俄开展特别军事行动\\\n",
    "    首先是因为其设定的目标未能通过外交方式实现。他表示，社会调查显示，四分之三的俄罗斯民众支持在乌开展特别军事行动。西方国家企\\\n",
    "    图通过制裁影响俄民众，煽动他们反对政府，结果适得其反。\"\n",
    "jieba.analyse.textrank(text, topK=20, withWeight=True, allowPOS=('ns', 'n', 'vn', 'v','nr', 'nt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结合主题的LDA算法\n",
    "- `sklearn`库和`genism`库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 英文语料库\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "# Define the word list\n",
    "word_list = [[\"running\", \"cars\"],[\"better\", \"driving\"]]\n",
    "\n",
    "# Lemmatize list of words and join\n",
    "def lemmatize_sentence(list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tagged_sentence = pos_tag(list)\n",
    "    lemmatized_sentence = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tagged_sentence]\n",
    "    return lemmatized_sentence\n",
    "\n",
    "# Lemmatize the word list\n",
    "lemmatized_output = [lemmatize_sentence(word) for word in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Create the term dictionary of our corpus, where every unique term is assigned an index\n",
    "dictionary = corpora.Dictionary(lemmatized_output)\n",
    "\n",
    "# Convert list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in lemmatized_output]\n",
    "\n",
    "# Create a TF-IDF model from the corpus\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "# Transform the corpus to TF-IDF\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import CoherenceModel\n",
    "from tqdm import tqdm\n",
    "# Create empty lists to store coherence and perplexity values\n",
    "coherence_values = []\n",
    "perplexity_values = []\n",
    "range_topics = range(1, 6)\n",
    "\n",
    "# Loop through different numbers of topics\n",
    "for num_topics in tqdm(range_topics):\n",
    "    # Train LDA model with the given number of topics\n",
    "    ldamodel = LdaModel(corpus_tfidf, num_topics=num_topics, id2word=dictionary, passes=50)\n",
    "    # Calculate coherence score\n",
    "    coherence_model = CoherenceModel(model=ldamodel, texts=lemmatized_output, dictionary=dictionary, coherence='c_v')\n",
    "    coherence = coherence_model.get_coherence()\n",
    "    # Calculate perplexity\n",
    "    perplexity = ldamodel.log_perplexity(corpus_tfidf)\n",
    "    # Append coherence and perplexity values to the lists\n",
    "    coherence_values.append(coherence)\n",
    "    perplexity_values.append(perplexity)\n",
    "\n",
    "# Plot the relationship between number of topics and coherence values\n",
    "fig, ax1 = plt.subplots()\n",
    "line1, = ax1.plot(range_topics, coherence_values, label='Coherence', color='blue')\n",
    "ax1.set_xlabel('Number of Topics')\n",
    "ax1.set_ylabel('Coherence', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Create a second y-axis for perplexity values\n",
    "ax2 = ax1.twinx()\n",
    "line2, = ax2.plot(range_topics, perplexity_values, label='Perplexity', color='red')\n",
    "ax2.set_ylabel('Perplexity', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Set title and legend\n",
    "plt.title('Coherence and Perplexity vs. Number of Topics')\n",
    "\n",
    "# Combine legends\n",
    "lines = [line1, line2]\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc='upper right')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结合语义编码的KeyBert算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "import jieba \n",
    "\n",
    "# model = KeyBERT('bert-base-chinese')\n",
    "# text = \"美国总统拜登当地时间26日又语出惊人。据路透社报道，拜登当天在波兰的演 讲中，称俄罗斯总统普京“不能继续掌权了”。\\\n",
    "# 白宫官员随后出来解释，克里姆林宫同日也作出回应，称“这不是由拜登决定的”。报道称，在华沙皇家城堡发表的讲话中，\\\n",
    "# 拜登在谴责普京后表示“看在上帝的份上，这个人不能继续掌权了”。对此，路透社表示，该言论引发了华盛顿方面对局势升级的担忧，\\\n",
    "# 美国一直避免直接对乌克兰进行军事干预，并明确表示不支持政权更迭。随后，一名白宫官员对拜登这番话进行解释，称拜登的言论并不代表华盛顿政策的转变，\\\n",
    "# 其目的是让世界为乌克兰问题上的长期冲突做好准备。该官员称，拜登意为“不能 允许普京对其邻国或该地区行使权力”，\\\n",
    "# 而不是在讨论普京在俄罗斯的权力或俄罗斯的政权 更迭问题。同日，路透社称，被问及拜登这一言论时，俄罗斯总统新闻秘书佩斯科夫作出回应，\\\n",
    "# 称“这不是由拜登决定的。俄罗斯总统是由俄罗斯人选举产生的”。此前，俄罗斯安全会议副主席梅德韦杰夫接受俄媒采访时曾表示，\\\n",
    "# 俄特别军事行动既定目标包括实现乌克兰去军事化、去纳粹化、成为中立国家、不奉行反俄政策。俄开展特别军事行动首先是因为其设定的目标未能通过外交方式实现。\\\n",
    "# 他表示，社会调查显示，四分之三的俄罗斯民众支持在乌开展特别军事行动。西方国家企图通过制裁影响俄民众，煽动他们反对政府，结果适得其反。\"\n",
    "# doc = \" \".join(jieba.cut(text))\n",
    "\n",
    "model = KeyBERT('bert-base-uncased')\n",
    "doc = \"I like it very much\"\n",
    "keywords = model.extract_keywords(doc, keyphrase_ngram_range=(1,2),  top_n=20)\n",
    "keywords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
