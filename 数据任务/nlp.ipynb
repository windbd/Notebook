{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字符串处理\n",
    "- 正则表达式用于字符或字符串匹配，可以查找和替换字符或字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World! I am Amit Chauhan\n",
      "Happy Good Home\n"
     ]
    }
   ],
   "source": [
    "#A sentence and the removing character from the sentence\n",
    "sentence = \"****Hello World! I am Amit Chauhan****\"\n",
    "removing_character = \"*\"\n",
    "#using strip function to remove star(*)\n",
    "print(sentence.strip(removing_character))\n",
    "\n",
    "str1 = \"Happy\"\n",
    "str2 = \"Home\"\n",
    "print(\" Good \".join([str1, str2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(20, 24), match='very'>\n",
      "(20, 24)\n",
      "(47, 51)\n"
     ]
    }
   ],
   "source": [
    "# to use a regular expression, we need to import re\n",
    "import re\n",
    "sentence = \"My computer gives a very good performance in a very short time.\"\n",
    "string = \"very\"\n",
    "str_match = re.search(string, sentence)\n",
    "print(str_match) \n",
    "for word in re.finditer(\"very\", sentence): \n",
    "  print(word.span())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词和词干"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词 Tokenization\n",
    "- 句子在前缀，中缀，后缀和异常中分开\n",
    "- nltk和spacy都提供了分词方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"'m\", 'going', 'to', 'meet\\\\', 'M.S', '.', 'Dhoni', '.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "example_string = \"I'm going to meet\\ M.S. Dhoni.\"\n",
    "words = nltk.word_tokenize(example_string)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I，'m，going，to，meet\\，M.S.，Dhoni，.，"
     ]
    },
    {
     "data": {
      "text/plain": [
       "going to meet\\ M.S."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "#Loading spacy english library\n",
    "load_en = spacy.load('en_core_web_sm')\n",
    "#take an example of string\n",
    "example_string = \"I'm going to meet\\ M.S. Dhoni.\"\n",
    "#load string to library \n",
    "words = load_en(example_string)\n",
    "#getting tokens pieces with for loop\n",
    "for tokens in words:\n",
    "    print(tokens.text,end='\\t')\n",
    "words[2:6]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 抽干 Stemming\n",
    "- 将单词还原为词根或词干的含义的过程\n",
    "- 去除单词的前后缀得到词根的过程，用于扩展检索，粒度较粗；Porter，「推荐」Snowball，Lancaster\n",
    "- Spacy不包含词干提取器，需要使用NLTK库进行词干提取过程 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy----->happi\n",
      "happier----->happier\n",
      "happiest----->happiest\n",
      "happiness----->happi\n",
      "breathing----->breath\n",
      "fairly----->fairli\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#import porter stemmer from nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "pot_stem = PorterStemmer()\n",
    "#random words to test porter stemmer\n",
    "words = ['happy', 'happier', 'happiest', 'happiness', 'breathing', 'fairly']\n",
    "for word in words:\n",
    "    print(word + '----->' + pot_stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy----->happi\n",
      "happier----->happier\n",
      "happiest----->happiest\n",
      "happiness----->happi\n",
      "breathing----->breath\n",
      "fairly----->fair\n"
     ]
    }
   ],
   "source": [
    "#Snowball提取器用于更改进的方法\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snow_stem = SnowballStemmer(language='english')\n",
    "for word in words:\n",
    "    print(word + '----->' + snow_stem.stem(word))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拔词和停用词"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词性归并 Lemmatization \n",
    "- 基于词典将单词的复杂形态转变成最基础的形态，用于更细粒度、更为准确的文本分析和表达\n",
    "- 词法化胜于词干，它能提供丰富的信息，这就是为什么spacy具有词法化而不是抽词干的原因\n",
    "- nltk也可以Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\t'm\thappy\tin\tthis\thappiest\tplace\twith\tall\thappiness\t.\tIt\tfeel\thow\thappier\twe\tare\t"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "__description__:词形还原Lemmatization\n",
    "\"\"\"\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    " \n",
    "wnl = WordNetLemmatizer()\n",
    "# 获取单词的词性\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# 分别定义需要进行还原的单词与相对应的词性\n",
    "example_string = \"I'm happy in this happiest place with all happiness. It feels how happier we are\"\n",
    "#load string to library \n",
    "words = nltk.word_tokenize(example_string)\n",
    "for i in range(len(words)):\n",
    "    print(wnl.lemmatize(words[i]),end='\\t')\n",
    "    # print(words[i]+'--'+get_wordnet_pos(pos_tag([words[i]])[0][1])+'-->'\n",
    "    #       +wnl.lemmatize(words[i],get_wordnet_pos(pos_tag([words[i]])[0][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t 4690420944186131903 \t I\n",
      "'m \t AUX \t 10382539506755952630 \t be\n",
      "happy \t ADJ \t 244022080605231780 \t happy\n",
      "in \t ADP \t 3002984154512732771 \t in\n",
      "this \t DET \t 1995909169258310477 \t this\n",
      "happiest \t ADJ \t 244022080605231780 \t happy\n",
      "place \t NOUN \t 7512738811199700769 \t place\n",
      "with \t ADP \t 12510949447758279278 \t with\n",
      "all \t DET \t 13409319323822384369 \t all\n",
      "happiness \t NOUN \t 2779265004918961325 \t happiness\n",
      ". \t PUNCT \t 12646065887601541794 \t .\n",
      "It \t PRON \t 10239237003504588839 \t it\n",
      "feels \t VERB \t 5741770584995928333 \t feel\n",
      "how \t SCONJ \t 16331095434822636218 \t how\n",
      "happier \t ADJ \t 244022080605231780 \t happy\n",
      "we \t PRON \t 16064069575701507746 \t we\n",
      "are \t AUX \t 10382539506755952630 \t be\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "#Loading spacy english library\n",
    "load_en = spacy.load('en_core_web_sm')\n",
    "#take an example of string\n",
    "example_string = load_en(u\"I'm happy in this happiest place with all happiness. It feels how happier we are\")\n",
    "for lem_word in example_string:\n",
    "    print(lem_word.text, '\\t', lem_word.pos_, '\\t', lem_word.lemma, '\\t', lem_word.lemma_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 停止词\n",
    "- Spacy中，有一些停用词的内置列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'being', 'must', 'after', 'some', 'have', 'be', 'both', 'though', 'along', 'could', 'thereby', 're', 'hereupon', 'nobody', 'within', 'forty', 'still', 'thus', 'ten', 'everyone', 'third', 'again', 'whatever', 'nor', 'here', 'herself', 'her', 'each', 'would', 'ca', 'none', 'whoever', 'keep', 'therefore', 'that', 'former', 'between', 'does', '’ve', 'moreover', \"'d\", 'about', 'on', \"n't\", 'except', 'already', 'mostly', 'out', 'either', 'are', 'this', 'whether', 'toward', 'alone', 'am', 'throughout', 'anything', 'thereafter', 'take', 'than', 'thereupon', 'say', 'these', 'it', 'via', 'nevertheless', 'they', 'eleven', 'becomes', 'someone', 'nine', 'top', 'every', 'which', 'fifty', 'doing', 'therein', 'only', 'whither', '‘re', 'your', 'indeed', 'without', 'eight', 'never', 'such', 'six', 'in', 'i', 'can', 'who', 'everything', 'by', 'enough', 'often', 'otherwise', 'whole', 'even', 'had', 'he', 'below', 'ourselves', 'else', 'full', 'n’t', 'more', 'whereby', 'how', 'make', 'then', 'we', 'hundred', 'at', 'around', 'them', 'formerly', 'amount', 'do', 'cannot', 'nothing', 'another', 'herein', 'from', 'hereafter', 'namely', 'whenever', 'beforehand', 'above', 'she', 'up', 'few', 'whence', 'if', 'itself', 'seeming', 'should', 'against', 'anywhere', 'may', '’ll', 'becoming', 'onto', 'afterwards', 'always', 'perhaps', 'under', 'using', 'get', 'for', '‘ll', 'four', 'rather', 'before', 'not', 'whereafter', 'beside', 'show', 'ours', 'hence', 'besides', 'among', 'will', 'front', 'their', \"'ll\", 'as', 'next', 'last', 'hereby', 'unless', 'ever', 'least', 'almost', 'has', 'latterly', 'us', 'one', 'or', 'once', 'something', 'mine', 'really', 'our', 'used', 'wherever', '’re', 'its', 'much', 'elsewhere', 'over', '‘s', 'others', \"'re\", 'because', 'anyhow', 'many', 'n‘t', 'fifteen', 'amongst', 'side', '’s', 'been', 'somewhere', 'own', 'any', 'down', 'whose', 'yours', 'serious', 'anyone', 'see', 'me', 'seem', 'themselves', '‘ve', 'everywhere', 'might', 'become', 'wherein', 'yourself', 'sometimes', '‘d', 'give', 'same', 'hers', 'no', 'the', 'less', 'sixty', 'noone', 'regarding', '’m', 'whom', 'go', 'since', \"'m\", 'an', 'put', 'five', 'twelve', 'himself', 'upon', 'meanwhile', 'whereupon', 'various', 'through', \"'ve\", 'thence', 'per', 'very', 'several', 'myself', '’d', 'of', \"'s\", 'however', 'were', 'empty', 'my', 'first', 'just', 'well', 'most', 'other', 'twenty', 'now', 'sometime', '‘m', 'nowhere', 'due', 'and', 'became', 'was', 'too', 'yourselves', 'is', 'but', 'back', 'latter', 'behind', 'all', 'his', 'so', 'with', 'quite', 'yet', 'during', 'seemed', 'done', 'thru', 'did', 'towards', 'please', 'where', 'whereas', 'three', 'although', 'made', 'off', 'anyway', 'why', 'while', 'a', 'him', 'somehow', 'you', 'name', 'to', 'across', 'part', 'call', 'what', 'those', 'seems', 'there', 'further', 'when', 'also', 'together', 'until', 'bottom', 'neither', 'beyond', 'into', 'move', 'two'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "#Loading spacy english library\n",
    "load_en = spacy.load('en_core_web_sm')\n",
    "print(load_en.Defaults.stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词性和命名实体识别"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词性（POS）\n",
    "- 获取有关文本和单词的信息作为标记的过程\n",
    "- 存在动词粗标签和细粒度标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN\n",
      "NN\n",
      "{90: 1, 92: 1, 100: 1, 85: 1, 96: 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ADP'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "#Loading spacy english library\n",
    "load_en = spacy.load('en_core_web_sm')\n",
    "str1 = load_en(u\"This laptop belongs to Amit Chauhan\")\n",
    "#pos_ tag operation \n",
    "print(str1[1].pos_)\n",
    "#to know fine grained information\n",
    "print(str1[1].tag_)\n",
    "# 带有伪造的POS计数\n",
    "pos_count = str1.count_by(spacy.attrs.POS)\n",
    "print(pos_count)\n",
    "str1.vocab[85].text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 命名实体识别（NER）\n",
    "- NER可以标记文本的实体类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India - GPE - Countries, cities, states\n",
      "Studying - GPE - Countries, cities, states\n",
      "IIT - ORG - Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "#Loading spacy english library\n",
    "load_en = spacy.load('en_core_web_sm')\n",
    "#lets label the entity in the text file\n",
    "file = load_en(u\" My girlFriend is living in India, Studying in IIT\")\n",
    "doc = file\n",
    "if doc.ents:\n",
    "    for ner in doc.ents:\n",
    "        print(ner.text + ' - '+ ner.label_ + ' - ' + str(spacy.explain(ner.label_)))\n",
    "else:\n",
    "    print('No Entity Found')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据清理与规范化\n",
    "- 深度学习中可能不需要该步骤\n",
    "- 常见预处理步骤：删除标点符号、表情符号、停用词、转小写、词干、提取主题等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import emoji\n",
    "import string\n",
    "\n",
    "def preprocess_text(text, remove_stop = True, stem_words = False, remove_mentions_hashtags = True):\n",
    "    \"\"\"\n",
    "    eg:\n",
    "    input: preprocess_text(\"@water #dream hi hello where are you going be there tomorrow happening happen happens\",  \n",
    "    stem_words = True) \n",
    "    output: ['tomorrow', 'happen', 'go', 'hello']\n",
    "    \"\"\"\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\"[\" \"\\U0001F1E0-\\U0001F6FF\" \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r\"\", text)\n",
    "    text = \"\".join([x for x in text if x not in emoji.UNICODE_EMOJI])\n",
    "\n",
    "    if remove_mentions_hashtags:\n",
    "        text = re.sub(r\"@(\\w+)\", \" \", text)\n",
    "        text = re.sub(r\"#(\\w+)\", \" \", text)\n",
    "        \n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
    "    nopunct = regex.sub(\" \", text.lower()) #lower-case\n",
    "    words = (''.join(nopunct)).split()\n",
    "    \n",
    "    if(remove_stop):\n",
    "        words = [w for w in words if w not in ENGLISH_STOP_WORDS]\n",
    "        words = [w for w in words if len(w) > 2]  # remove a,an,of etc.\n",
    "        \n",
    "    if(stem_words):\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "        \n",
    "    return list(words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量（word vector）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 传统文本表示方法\n",
    "**One-hot**  \n",
    "将每一个*单词*使用一个离散的向量表示\n",
    "首先对所有句子的字进行索引，即将每个字确定一个编号：\n",
    "```python\n",
    "句子1：我 爱 北 京 天 安 门\n",
    "句子2：我 喜 欢 上 海\n",
    "{'我': 1, '爱': 2, '北': 3, '京': 4, '天': 5,\n",
    "'安': 6, '门': 7, '喜': 8, '欢': 9, '上': 10, '海': 11}\n",
    "```\n",
    "在这里共包括11个字，因此每个字可以转换为一个11维度稀疏向量：\n",
    "```\n",
    "我：[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "爱：[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "...\n",
    "海：[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "```\n",
    "\n",
    "**Bag of Words**  \n",
    "Bag of Words（词袋表示），也称为`Count Vectors`，每个文档的字/词可以使用其*出现次数*来进行表示\n",
    "直接统计每个字出现的次数，并进行赋值：\n",
    "```python\n",
    "句子1：我 爱 北 京 天 安 门\n",
    "转换为 [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "\n",
    "句子2：我 喜 欢 上 海\n",
    "转换为 [1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
    "```\n",
    "在sklearn中用`CountVectorizer`实现这一步骤\n",
    "\n",
    "**N-gram**  \n",
    "N-gram与Count Vectors类似，不过加入了相邻单词组合成为新的单词，并进行计数。\n",
    "如果N取值为2，则句子1和句子2就变为：\n",
    "```\n",
    "句子1：我爱 爱北 北京 京天 天安 安门\n",
    "句子2：我喜 喜欢 欢上 上海\n",
    "```\n",
    "\n",
    " **TF-IDF**  \n",
    " TF-IDF 分数由两部分组成：第一部分是*词语频率*（Term Frequency），第二部分是*逆文档频率*（Inverse Document Frequency）\n",
    " \n",
    " $TF=\\frac{该词语在当前文档出现的次数}{当前文档中词语的总数}\\\\\n",
    " IDF=\\log{(\\frac{文档总数}{出现该词语的文档总数})}\\\\\n",
    " TF-IDF=TF*IDF$\n",
    "\n",
    " 在sklearn中用`TfidfVectorizer`实现这一步骤\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today', 'good', 'day', 'tomorrow', 'going', 'good', 'day']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = 'Today is a good day,tomorrow is going to be a good day'\n",
    "vocab = preprocess_text(corpus)\n",
    "vocab\n",
    "# ‘Tomorrow will be a good day’的一天的文本可以编码为：[0,1,1,0,1,1,0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词频矩阵：\n",
      " [[1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 1]\n",
      " [1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1]\n",
      " [1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1]\n",
      " [1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1]]\n",
      "TF-IDF矩阵：\n",
      " [[0.26147089 0.         0.50105424 0.26147089 0.         0.26147089\n",
      "  0.39503692 0.26147089 0.         0.         0.         0.\n",
      "  0.         0.         0.50105424 0.26147089]\n",
      " [0.24987111 0.47882569 0.         0.24987111 0.         0.24987111\n",
      "  0.         0.24987111 0.         0.         0.         0.47882569\n",
      "  0.47882569 0.         0.         0.24987111]\n",
      " [0.24318358 0.         0.         0.24318358 0.46601044 0.24318358\n",
      "  0.36740799 0.24318358 0.46601044 0.         0.         0.\n",
      "  0.         0.36740799 0.         0.24318358]\n",
      " [0.26147089 0.         0.         0.26147089 0.         0.26147089\n",
      "  0.         0.26147089 0.         0.50105424 0.50105424 0.\n",
      "  0.         0.39503692 0.         0.26147089]]\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 假设我们有以下的中文文档集合\n",
    "documents = [\n",
    "    \"今天天气不错，我们去郊游吧。\",\n",
    "    \"今天天气不好，我们去看电影吧。\",\n",
    "    \"今天天气真不错，我们去公园散步吧。\",\n",
    "    \"今天天气真不错，我们去海边玩吧。\"\n",
    "]\n",
    "\n",
    "# 使用jieba进行中文分词\n",
    "def chinese_tokenizer(text):\n",
    "    return jieba.cut(text)\n",
    "\n",
    "# 初始化CountVectorizer，使用jieba分词\n",
    "vectorizer = CountVectorizer(tokenizer=chinese_tokenizer)\n",
    "\n",
    "# 计算词频（Term Frequency）\n",
    "X_counts = vectorizer.fit_transform(documents)\n",
    "\n",
    "# 初始化TfidfVectorizer，使用jieba分词\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=chinese_tokenizer)\n",
    "\n",
    "# 计算TF-IDF权重\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# 打印词频矩阵\n",
    "print(\"词频矩阵：\\n\", X_counts.toarray())\n",
    "\n",
    "# 打印TF-IDF矩阵\n",
    "print(\"TF-IDF矩阵：\\n\", X_tfidf.toarray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词嵌入(Word Embedding)\n",
    "**FastText**\n",
    "- 通过Embedding层将单词映射到稠密空间，然后将句子中所有的单词在Embedding空间中进行平均\n",
    "- FastText用单词的Embedding叠加获得的文档向量，将相似的句子分为一类\n",
    "- FastText学习到的Embedding空间维度比较低，可以快速进行训练\n",
    "\n",
    "**Word2vec**\n",
    "- 使用浅层两层神经网络执行特定任务\n",
    "- CBOW（连续词袋）：根据源上下文单词（环绕单词）预测当前的目标单词（中心单词）\n",
    "- Skip—Gram：在给定目标单词（中心单词）的情况下预测源上下文单词（环绕单词）\n",
    "  \n",
    "**Glove** \n",
    "- 与Word2vec不同，Glove利用了单词的全局共现，而不仅仅是局部上下文\n",
    "- 有许多在大规模语料库经过预训练的，具有不同向量长度的预训练词向量，例如Glove，fasttext等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### word2vec\n",
    "# 使用词向量相似度查找与给定文档最相似的文档\n",
    "import numpy as np\n",
    "\n",
    "#loading the glove file into a dictionary of words\n",
    "def load_glove(filename):\n",
    "    glove_dict = {}\n",
    "    with open(filename) as f:\n",
    "        file_content = f.readlines()\n",
    "    for line in file_content:\n",
    "        line_content = line.split()\n",
    "        glove_dict[line_content[0]] = np.array(line_content[1:], dtype=float)\n",
    "    return glove_dict\n",
    "\n",
    "#get centroid of a particular document\n",
    "def get_centroid(text, gloves):\n",
    "    words_list = preprocess_text(text)\n",
    "    word_vec_sum = 0\n",
    "    words_count = 0\n",
    "    for w in words_list:\n",
    "        if w in gloves:\n",
    "            word_vec_sum += gloves[w]\n",
    "            words_count += 1\n",
    "    if words_count:\n",
    "        return word_vec_sum/words_count\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#get distance between two centroids\n",
    "def get_distance (a,b):\n",
    "    return (np.linalg.norm(a - b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  ['potato', 'century', 'potatoes', 'plant', 'irish', 'crop', 'major', 'crops', 'end', 'tubers', 'europe', 'baking', 'flour', 'used', 'thickener', 'vegetable', 'sauces', 'highly', 'digestible', 'supply', 'vitamin', 'protein', 'thiamin', 'niacin', 'thought', 'ground', 'served', 'cooked', 'edible', 'solanum', 'tuberosum', 'annual', 'nightshade', 'family', 'solanaceae', 'grown', 'starchy', 'native', 'mashed', 'peruvian', 'bolivian', 'andes', 'world', 'main', 'food', 'frequently', 'domesticated', 'independently', 'dependence', 'times', 'mid', 'western', 'eastern', 'hemispheres', 'decades', 'economy', 'dependent', 'disastrous', 'failures', 'especially', 'largely', 'late', 'blight', 'phytophthora', 'infestans', 'resulting', 'famine', 'generated', 'cautious', 'spread', 'continued', 'england', 'west', 'cultivated', 'attitude', 'america', 'incas', 'early', 'years', 'ago', 'encountered', 'invading', 'spaniards', 'introduced', 'second', 'half', 'ireland', 'continental', 'particularly', 'germany', 'south'] \n",
      "\n",
      "[('incas', 0.19040964543819427), ('late', 0.17752844095230103), ('peruvian', 0.16140559315681458), ('america', 0.1361526995897293), ('century', 0.13086725771427155), ('end', 0.12760989367961884), ('south', 0.11991690844297409), ('invading', 0.11883427947759628), ('generated', 0.11808259040117264), ('protein', 0.11351343244314194)] \n",
      "\n",
      "[-3.44675151e-04  1.49946049e-04  3.39044677e-03  5.98883256e-03\n",
      " -6.21718727e-03 -4.73772502e-03  4.29294864e-03  5.99058811e-03\n",
      " -3.34663969e-03 -2.50369357e-03  4.92450502e-03 -1.03078980e-03\n",
      " -3.04480433e-03  4.41653794e-03 -3.21856281e-03 -1.21052482e-03\n",
      "  1.92473771e-03  6.58238248e-04 -5.54655399e-03 -6.29399996e-03\n",
      "  4.85484349e-03  3.37493652e-03  4.51729028e-03  5.13455248e-04\n",
      "  4.22111340e-03 -2.26735580e-03 -6.59737852e-04  3.84788564e-03\n",
      " -5.02095977e-03 -2.64597405e-03 -5.03511447e-03 -6.10484334e-04\n",
      "  6.33930648e-03 -4.88228537e-03 -1.56191678e-03 -1.27618934e-03\n",
      "  5.41846128e-03 -3.94348754e-03  2.86721825e-05 -3.16787581e-03\n",
      " -6.38063392e-03  3.35897761e-03 -5.87598514e-03 -2.92611122e-03\n",
      " -2.93677044e-06 -1.96103923e-04 -5.10935392e-03  6.40203897e-03\n",
      "  3.32589494e-03  6.16138708e-03 -5.45503944e-03  2.98420270e-03\n",
      " -2.74320436e-03  5.47572796e-04  5.70455473e-03 -2.95481831e-03\n",
      "  3.00439843e-03 -4.50858753e-03 -2.32807267e-03  6.26533665e-03\n",
      " -1.03559636e-03  2.04863798e-04 -2.75516440e-03 -5.10612037e-03\n",
      " -9.84845334e-04  1.67536491e-03 -5.68636984e-04  3.66217503e-03\n",
      " -1.84841931e-03  1.50491053e-03  3.65618942e-03  5.59302792e-03\n",
      " -9.42006649e-04 -6.13948936e-03  2.91921431e-03  3.89822613e-04\n",
      "  4.97247605e-03 -5.39902772e-04 -1.76744279e-03 -5.86589379e-03\n",
      " -5.64077927e-04  1.86859863e-03  3.61132901e-03  4.75566089e-03\n",
      " -3.81552801e-03  1.23432325e-03  4.08432586e-03 -3.19639500e-03\n",
      " -2.05978681e-03  4.53189760e-03  1.11677311e-03  1.19260025e-04\n",
      "  2.30740570e-03  1.49243846e-04  6.42367732e-03  3.38688330e-03\n",
      " -5.95139852e-03 -4.70333314e-03  6.13709562e-04  4.28600190e-03\n",
      " -5.74950455e-03  2.47104396e-03  3.46717425e-03  3.85242584e-03\n",
      "  4.97450633e-03 -4.12999513e-03  7.57795759e-04  4.05891798e-03\n",
      " -1.92591688e-03 -4.11412492e-03 -3.05178692e-04 -5.58316195e-03\n",
      " -3.72315804e-03  4.75159846e-03  2.21170671e-03  4.82315291e-03\n",
      "  4.52918606e-03  5.01675205e-03 -2.50546006e-03 -3.71084869e-04\n",
      "  1.57795195e-03 -3.00972699e-03  5.57583850e-03 -6.56740181e-03\n",
      "  4.50831931e-03  1.96436211e-03 -3.27847945e-03  2.91441148e-03\n",
      " -1.17303000e-03  4.49053245e-03  6.66111195e-03 -2.88428902e-03\n",
      " -3.96368676e-04 -3.81438807e-03  2.54848180e-03  1.88973616e-03\n",
      "  4.57437523e-03  4.07206500e-03  6.33885153e-03  6.14604400e-03\n",
      "  5.28156012e-03 -4.66401502e-03 -6.10478036e-03 -2.34709732e-04\n",
      " -2.05805618e-03  5.26113762e-03  3.96429049e-03 -1.03279192e-03\n",
      "  1.00759661e-03  1.16039731e-03] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  训练从头开始生成单词向量\n",
    "import gensim\n",
    "words_list = []\n",
    "with open('../data/potato.txt') as f:\n",
    "    for text in f:\n",
    "        words_list.append(preprocess_text(text))\n",
    "model = gensim.models.Word2Vec(\n",
    "        words_list,\n",
    "        vector_size=150,\n",
    "        window=2,\n",
    "        min_count=1,\n",
    "        workers=10,\n",
    "        epochs=10)\n",
    "print('vocabulary: ', model.wv.index_to_key,'\\n')\n",
    "print(model.wv.most_similar('starchy'),'\\n')\n",
    "print(model.wv.get_vector('potato'),'\\n')\n",
    "len(model.wv.index_to_key)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
